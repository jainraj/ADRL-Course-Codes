{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338f6775",
   "metadata": {},
   "source": [
    "### General Settings\n",
    "\n",
    "Change the respective settings to run appropriately\n",
    "\n",
    "Use `limit_train_batches`, `limit_val_batches`, `limit_test_batches` as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434ce38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/Users/rajjain/PycharmProjects/ADRL-Course-Work/'\n",
    "cifar_data_dir = '/Users/rajjain/Desktop/CourseWork/Cifar/'\n",
    "use_gpu = False\n",
    "num_cpus = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b982b35",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96a6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Sequential, Flatten, Module, init, CrossEntropyLoss, ReLU, BCEWithLogitsLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.nn.functional import normalize\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from typing import Callable, Optional\n",
    "from torchvision import transforms\n",
    "from torch.optim import SGD, Adam\n",
    "from torchinfo import summary\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import numpy\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de3428",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd62aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_class_mapper = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 4,\n",
    "    5: 3,\n",
    "    6: 4,\n",
    "    7: 2,\n",
    "    8: 0,\n",
    "    9: 1,\n",
    "}\n",
    "\n",
    "two_class_mapper = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 1,\n",
    "    6: 1,\n",
    "    7: 0,\n",
    "    8: 0,\n",
    "    9: 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f8032",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ef9ee",
   "metadata": {},
   "source": [
    "## Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4cea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Module):\n",
    "    \"\"\"Feature extractor for CIFAR-10 dataset\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        backbone = resnet50(pretrained=True)\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        self.model = Sequential(\n",
    "            *layers,\n",
    "            Flatten(),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # 2048 dim vectors\n",
    "\n",
    "\n",
    "class OutputLayer(Module):\n",
    "    \"\"\"Get multi-dimensional output from features\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(OutputLayer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.model = Sequential(\n",
    "            Linear(in_features=2048, out_features=num_classes if num_classes > 2 else 1)\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        init.xavier_uniform_(self.model[0].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Classifier(LightningModule):\n",
    "\n",
    "    def __init__(self, feature_extractor: Optional[FeatureExtractor], num_classes: int,\n",
    "                 target_transform: Optional[Callable], bs: int):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.target_transform = target_transform\n",
    "        self.bs = bs\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        seed_everything(0)\n",
    "\n",
    "        self.output_layer = OutputLayer(num_classes)  # always train the output layer\n",
    "        self.output_layer.initialise()\n",
    "\n",
    "        if feature_extractor:  # if provided, don't train\n",
    "            self.train_features = False\n",
    "            self.feature_extractor = FeatureExtractor()\n",
    "            self.feature_extractor.load_state_dict(feature_extractor.state_dict())\n",
    "            self.feature_extractor.requires_grad_(False)\n",
    "        else:\n",
    "            self.train_features = True\n",
    "            self.feature_extractor = FeatureExtractor()\n",
    "            self.feature_extractor.initialise()  # by default requires grad is true here\n",
    "\n",
    "        self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.output_layer(features)\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        not_training = btype != 'train'\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        if self.output_layer.num_classes > 2:\n",
    "            loss = CrossEntropyLoss()(y_hat, y)\n",
    "        else:\n",
    "            loss = BCEWithLogitsLoss()(y_hat[:, 0], y.float())\n",
    "        acc = accuracy(y_hat, y, average='macro', num_classes=self.output_layer.num_classes, multiclass=True)\n",
    "        self.log(f'{btype}/loss', loss, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/acc', acc, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.feature_extractor.train(self.train_features)  # Train feature extractor only if to be trained\n",
    "        return self._common_step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds = CIFAR10(cifar_data_dir, train=True,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                     ]),\n",
    "                     target_transform=self.target_transform)\n",
    "        dl = DataLoader(ds, self.bs, shuffle=True, num_workers=num_cpus)\n",
    "        return dl\n",
    "\n",
    "    def eval_dataloader(self):\n",
    "        ds = CIFAR10(cifar_data_dir, train=False,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                     ]),\n",
    "                     target_transform=self.target_transform)\n",
    "        dl = DataLoader(ds, self.bs, shuffle=False, num_workers=num_cpus)\n",
    "        return dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.eval_dataloader()\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.eval_dataloader()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, step_size_up=10, mode='triangular2',\n",
    "        #          cycle_momentum=False)\n",
    "        # CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs)\n",
    "        # ExponentialLR(optimizer, gamma=0.9)\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=1e-4,\n",
    "                                         threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-8)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val/loss',\n",
    "                'name': 'learning_rate',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        summary_kwargs = dict(dtypes=[torch.float], depth=3, col_names=['input_size', 'output_size', 'num_params'],\n",
    "                              row_settings=['depth', 'var_names'], verbose=0, device=torch.device('cpu'))\n",
    "        imgs = torch.randn((10, 3, 32, 32), dtype=torch.float)\n",
    "        summary_string = str(summary(model=self, input_data=imgs, **summary_kwargs))\n",
    "        return summary_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da79d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                            Input Shape               Output Shape              Param #\n",
      "=======================================================================================================================================\n",
      "Classifier (Classifier)                                      [10, 3, 32, 32]           [10, 10]                  --\n",
      "├─FeatureExtractor (feature_extractor): 1-1                  [10, 3, 32, 32]           [10, 2048]                --\n",
      "│    └─Sequential (model): 2-1                               [10, 3, 32, 32]           [10, 2048]                --\n",
      "│    │    └─Conv2d (0): 3-1                                  [10, 3, 32, 32]           [10, 64, 16, 16]          9,408\n",
      "│    │    └─BatchNorm2d (1): 3-2                             [10, 64, 16, 16]          [10, 64, 16, 16]          128\n",
      "│    │    └─ReLU (2): 3-3                                    [10, 64, 16, 16]          [10, 64, 16, 16]          --\n",
      "│    │    └─MaxPool2d (3): 3-4                               [10, 64, 16, 16]          [10, 64, 8, 8]            --\n",
      "│    │    └─Sequential (4): 3-5                              [10, 64, 8, 8]            [10, 256, 8, 8]           215,808\n",
      "│    │    └─Sequential (5): 3-6                              [10, 256, 8, 8]           [10, 512, 4, 4]           1,219,584\n",
      "│    │    └─Sequential (6): 3-7                              [10, 512, 4, 4]           [10, 1024, 2, 2]          7,098,368\n",
      "│    │    └─Sequential (7): 3-8                              [10, 1024, 2, 2]          [10, 2048, 1, 1]          14,964,736\n",
      "│    │    └─AdaptiveAvgPool2d (8): 3-9                       [10, 2048, 1, 1]          [10, 2048, 1, 1]          --\n",
      "│    │    └─Flatten (9): 3-10                                [10, 2048, 1, 1]          [10, 2048]                --\n",
      "├─OutputLayer (output_layer): 1-2                            [10, 2048]                [10, 10]                  --\n",
      "│    └─Sequential (model): 2-2                               [10, 2048]                [10, 10]                  --\n",
      "│    │    └─Linear (0): 3-11                                 [10, 2048]                [10, 10]                  20,490\n",
      "=======================================================================================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 834.85\n",
      "=======================================================================================================================================\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 36.29\n",
      "Params size (MB): 94.11\n",
      "Estimated Total Size (MB): 130.53\n",
      "=======================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(Classifier(None, 10, None, 2).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e1501",
   "metadata": {},
   "source": [
    "## MoCo Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c515c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingTransforms:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.8),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        q = self.transform(inp)\n",
    "        k = self.transform(inp)\n",
    "        return q, k\n",
    "\n",
    "\n",
    "class TestTransforms:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        q = self.transform(inp)\n",
    "        k = self.transform(inp)\n",
    "        return q, k\n",
    "\n",
    "\n",
    "class MoCo(LightningModule):\n",
    "    \"\"\"Inspired from the original facebook implementation - We won't use multiple GPUs\"\"\"\n",
    "\n",
    "    m = 0.999  # moco momentum\n",
    "    tau = 0.07  # softmax temperature\n",
    "\n",
    "    def __init__(self, dict_size: int, bs: int):\n",
    "        super(MoCo, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.dict_size = dict_size\n",
    "        self.bs = bs\n",
    "\n",
    "        # FeatureExtractor will work as encoder. This gives a 2048-dim vector as output\n",
    "        self.encoder_q = FeatureExtractor()\n",
    "        self.encoder_k = FeatureExtractor()\n",
    "\n",
    "        seed_everything(0)\n",
    "\n",
    "        self.encoder_q.initialise()\n",
    "        self.encoder_k.load_state_dict(self.encoder_q.state_dict())\n",
    "        self.encoder_k.requires_grad_(False)\n",
    "\n",
    "        self.register_buffer(\"train_queue\", torch.randn(2048, dict_size))\n",
    "        self.train_queue = normalize(self.train_queue, dim=0)\n",
    "        self.register_buffer(\"train_queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "        self.register_buffer(\"val_queue\", torch.randn(2048, dict_size))\n",
    "        self.val_queue = normalize(self.val_queue, dim=0)\n",
    "        self.register_buffer(\"val_queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"Momentum update of the key encoder\"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys, queue, queue_ptr):\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(queue_ptr)\n",
    "        assert self.dict_size % batch_size == 0  # for simplicity\n",
    "        queue[:, ptr:ptr + batch_size] = keys.T  # replace the keys at ptr (dequeue and enqueue)\n",
    "        ptr = (ptr + batch_size) % self.dict_size  # move pointer\n",
    "        queue_ptr[0] = ptr\n",
    "\n",
    "    def forward(self, query_imgs, key_imgs, queue):\n",
    "        # compute query features\n",
    "        q = self.encoder_q(query_imgs)  # queries: NxC\n",
    "        q = normalize(q, dim=1)\n",
    "\n",
    "        # compute key features\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            k = self.encoder_k(key_imgs)  # keys: NxC\n",
    "            k = normalize(k, dim=1)\n",
    "\n",
    "        # compute logits - Einstein sum is more intuitive\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)  # positive logits: Nx1\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, queue.clone().detach()])  # negative logits: NxK\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)  # logits: Nx(1+K)\n",
    "        logits /= self.tau  # apply temperature\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=self.device)  # labels: positive key indicators\n",
    "\n",
    "        return logits, labels, k\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self._momentum_update_key_encoder()  # update the key encoder only during training\n",
    "\n",
    "        (query_imgs, key_imgs), _ = batch\n",
    "        output, target, keys = self(query_imgs, key_imgs, self.train_queue)\n",
    "        self._dequeue_and_enqueue(keys, self.train_queue, self.train_queue_ptr)\n",
    "\n",
    "        loss = CrossEntropyLoss()(output, target)\n",
    "        pred_class = torch.argmax(output, dim=1)\n",
    "        acc = (target == pred_class).cpu().numpy().mean()\n",
    "        self.log(f'train/loss', loss, on_step=False, on_epoch=True, sync_dist=False)\n",
    "        self.log(f'train/acc', acc, on_step=False, on_epoch=True, sync_dist=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (query_imgs, key_imgs), _ = batch\n",
    "        output, target, keys = self(query_imgs, key_imgs, self.val_queue)\n",
    "        self._dequeue_and_enqueue(keys, self.val_queue, self.val_queue_ptr)\n",
    "\n",
    "        loss = CrossEntropyLoss()(output, target)\n",
    "        pred_class = torch.argmax(output, dim=1)\n",
    "        acc = (target == pred_class).cpu().numpy().mean()\n",
    "        self.log(f'val/loss', loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(f'val/acc', acc, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = SGD(self.parameters(), lr=0.03, momentum=0.9, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingLR(optimizer, self.trainer.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds = CIFAR10(cifar_data_dir, train=True, transform=TrainingTransforms())\n",
    "        dl = DataLoader(ds, self.bs, shuffle=True, num_workers=0, drop_last=True)\n",
    "        return dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        ds = CIFAR10(cifar_data_dir, train=False, transform=TestTransforms())\n",
    "        dl = DataLoader(ds, self.bs, shuffle=True, num_workers=0, drop_last=True)\n",
    "        return dl\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        return ''\n",
    "\n",
    "\n",
    "class LinearClassifier(LightningModule):\n",
    "\n",
    "    def __init__(self, feature_extractor: FeatureExtractor, train_fraction: float, bs: int):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.train_fraction = train_fraction\n",
    "        self.bs = bs\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        seed_everything(0)\n",
    "\n",
    "        self.output_layer = OutputLayer(10)\n",
    "        self.output_layer.initialise()\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.feature_extractor.load_state_dict(feature_extractor.state_dict())\n",
    "        self.feature_extractor.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.output_layer(features)\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        not_training = btype != 'train'\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = CrossEntropyLoss()(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, average='macro', num_classes=self.output_layer.num_classes, multiclass=True)\n",
    "        self.log(f'{btype}/loss', loss, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/acc', acc, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.feature_extractor.eval()\n",
    "        return self._common_step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds = CIFAR10(cifar_data_dir, train=True,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                     ]))\n",
    "        total_samples = len(ds)\n",
    "        req_samples = int(total_samples * self.train_fraction)\n",
    "        rng = numpy.random.default_rng(0)\n",
    "        req_idxs = rng.permutation(total_samples)[:req_samples]\n",
    "        dl = DataLoader(ds, self.bs, num_workers=num_cpus, sampler=SubsetRandomSampler(req_idxs))\n",
    "        return dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        ds = CIFAR10(cifar_data_dir, train=False,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                     ]))\n",
    "        dl = DataLoader(ds, self.bs, shuffle=False, num_workers=num_cpus)\n",
    "        return dl\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=1e-4,\n",
    "                                         threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-8)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val/loss',\n",
    "                'name': 'learning_rate',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        summary_kwargs = dict(dtypes=[torch.float], depth=3, col_names=['input_size', 'output_size', 'num_params'],\n",
    "                              row_settings=['depth', 'var_names'], verbose=0, device=torch.device('cpu'))\n",
    "        imgs = torch.randn((10, 3, 32, 32), dtype=torch.float)\n",
    "        summary_string = str(summary(model=self, input_data=imgs, **summary_kwargs))\n",
    "        return summary_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef3950",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2359fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(max_epochs: int, tags: list[str], gpu_num: list[int],\n",
    "                   model_class, model_kwargs: dict, model_desc: str,\n",
    "                   limit_train_batches=1.0, limit_val_batches=1.0, limit_test_batches=1.0):\n",
    "    seed_everything(0, workers=True)\n",
    "\n",
    "    folder_name = f'run_{datetime.utcnow().isoformat(sep=\"T\", timespec=\"microseconds\")}'\n",
    "    results_dir = project_dir + f'self_supervised/results/{folder_name}/'\n",
    "    os.makedirs(results_dir, exist_ok=False)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val/loss', mode='min', dirpath=results_dir, filename='best',\n",
    "                                          save_last=True)\n",
    "    \n",
    "    model = model_class(**model_kwargs)\n",
    "\n",
    "    trainer_kwargs = dict(accelerator=\"gpu\", devices=gpu_num) if use_gpu else dict()\n",
    "    tf_logger = TensorBoardLogger(save_dir=results_dir, version=f'tf_logs', default_hp_metric=False)\n",
    "    trainer = Trainer(default_root_dir=results_dir, max_epochs=max_epochs, callbacks=[checkpoint_callback],\n",
    "                      logger=[tf_logger], log_every_n_steps=1, num_sanity_val_steps=0, deterministic=True,\n",
    "                      limit_train_batches=limit_train_batches, limit_val_batches=limit_val_batches,\n",
    "                      limit_test_batches=limit_test_batches,\n",
    "                      **trainer_kwargs)\n",
    "    trainer.fit(model)\n",
    "\n",
    "    summary = model.summary() + '\\n' + model_desc\n",
    "    with open(results_dir + 'model_desc.md', 'w') as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    gc.collect()\n",
    "    return folder_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44b27c",
   "metadata": {},
   "source": [
    "## Base Model Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741354e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | output_layer      | OutputLayer      | 20.5 K\n",
      "1 | feature_extractor | FeatureExtractor | 23.5 M\n",
      "-------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "/Users/rajjain/miniforge3/envs/gpfa_latents/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/rajjain/miniforge3/envs/gpfa_latents/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012120246887207031,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9024f26b3ac14e8393867ddbbaf61753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007557868957519531,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02750420570373535,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "/Users/rajjain/miniforge3/envs/gpfa_latents/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'feature_extractor' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['feature_extractor'])`.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | output_layer      | OutputLayer      | 10.2 K\n",
      "1 | feature_extractor | FeatureExtractor | 23.5 M\n",
      "-------------------------------------------------------\n",
      "10.2 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.073    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007689952850341797,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c0f3e9e73b45f293621f3157d1a26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007524967193603516,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.030844926834106445,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | output_layer      | OutputLayer      | 2.0 K \n",
      "1 | feature_extractor | FeatureExtractor | 23.5 M\n",
      "-------------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.040    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0075168609619140625,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0af1e51867e40feb3db7c399cd5b43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007819890975952148,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.032160043716430664,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_dir = train_and_test(max_epochs=2, tags=[], gpu_num=[], model_class=Classifier, \n",
    "                          model_kwargs=dict(feature_extractor=None, num_classes=10, target_transform=None, bs=10), \n",
    "                          model_desc='Base Model', limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n",
    "\n",
    "base_model = Classifier.load_from_checkpoint(project_dir + f'self_supervised/results/{base_dir}/best.ckpt')\n",
    "\n",
    "_ = train_and_test(max_epochs=2, tags=[], gpu_num=[], model_class=Classifier,\n",
    "                   model_kwargs=dict(feature_extractor=base_model.feature_extractor, num_classes=5, \n",
    "                                     target_transform=five_class_mapper.get, bs=10),\n",
    "                   model_desc='5-class Model', limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n",
    "\n",
    "_ = train_and_test(max_epochs=2, tags=[], gpu_num=[], model_class=Classifier,\n",
    "                   model_kwargs=dict(feature_extractor=base_model.feature_extractor, num_classes=2,\n",
    "                                     target_transform=two_class_mapper.get, bs=10),\n",
    "                   model_desc='2-class Model', limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0d6be",
   "metadata": {},
   "source": [
    "## MoCo Model Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529bdc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = 40\n",
    "train_fraction = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57fe56e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder_q | FeatureExtractor | 23.5 M\n",
      "1 | encoder_k | FeatureExtractor | 23.5 M\n",
      "-----------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "47.0 M    Total params\n",
      "188.064   Total estimated model params size (MB)\n",
      "/Users/rajjain/miniforge3/envs/gpfa_latents/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:495: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008148908615112305,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e932b2d79bd4dcd9d6eaba2a7bace0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007325887680053711,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007384777069091797,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | output_layer      | OutputLayer      | 20.5 K\n",
      "1 | feature_extractor | FeatureExtractor | 23.5 M\n",
      "-------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008006811141967773,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbd6cce7d7f4a90afbb24df64e65ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007508039474487305,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.032277822494506836,
       "initial": 0,
       "n": 0,
       "ncols": 153,
       "nrows": 12,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moco_dir = train_and_test(max_epochs=2, tags=[], gpu_num=[], model_class=MoCo,\n",
    "                          model_kwargs=dict(dict_size=dict_size, bs=10), model_desc='MoCo Model Training', \n",
    "                          limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n",
    "\n",
    "moco_model = MoCo.load_from_checkpoint(project_dir + f'self_supervised/results/{moco_dir}/best.ckpt')\n",
    "_ = train_and_test(max_epochs=2, tags=[], gpu_num=[], model_class=LinearClassifier,\n",
    "                   model_kwargs=dict(feature_extractor=moco_model.encoder_q, train_fraction=train_fraction, bs=10),\n",
    "                   model_desc='MoCo Classifier Training', \n",
    "                   limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29138f02",
   "metadata": {},
   "source": [
    "# t-SNE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b245b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "}\n",
    "\n",
    "\n",
    "def tsne_plot(model_dir, target_transform, num_classes):\n",
    "    bs = 50\n",
    "\n",
    "    ds = CIFAR10(cifar_data_dir, train=False,\n",
    "                 transform=transforms.Compose([\n",
    "                     transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                 ]),\n",
    "                 target_transform=target_transform)\n",
    "    dl = DataLoader(ds, bs, shuffle=False, num_workers=num_cpus)\n",
    "\n",
    "    model = Classifier.load_from_checkpoint(project_dir + f'self_supervised/results/{model_dir}/best.ckpt')\n",
    "    assert num_classes == model.output_layer.num_classes\n",
    "    model.eval()\n",
    "\n",
    "    all_features, all_ys = [], []\n",
    "    for x, y in tqdm(iter(dl)):\n",
    "        features = model.feature_extractor(x)\n",
    "        all_features.append(features.detach().numpy())\n",
    "        all_ys.append(y.numpy())\n",
    "\n",
    "    final_features = numpy.concatenate(all_features)\n",
    "    final_y = numpy.concatenate(all_ys)\n",
    "    if num_classes == 10:\n",
    "        final_y = list(map(mapper.get, final_y))\n",
    "        order = list(map(mapper.get, range(num_classes)))\n",
    "    else:\n",
    "        final_y = list(map(str, final_y))\n",
    "        order = list(map(str, range(num_classes)))\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0, init='pca', learning_rate='auto')\n",
    "    projections = tsne.fit_transform(final_features)\n",
    "    fig = px.scatter(projections, x=0, y=1, color=final_y,\n",
    "                     labels={'color': 'True Class', '0': 'Dimension 1', '1': 'Dimension 2'},\n",
    "                     category_orders={'color': order})\n",
    "    fig.update_layout(width=850, height=850, font_color=\"black\", font_size=15)\n",
    "    fig.update_xaxes(title_standoff=0)\n",
    "    fig.update_yaxes(title_standoff=0)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
