{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820b2634",
   "metadata": {},
   "source": [
    "### General Settings\n",
    "\n",
    "Change the respective settings to run appropriately\n",
    "\n",
    "Use `limit_train_batches`, `limit_val_batches`, `limit_test_batches` as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63714e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/Users/rajjain/PycharmProjects/ADRL-Course-Work/'\n",
    "data_dir = project_dir + 'data/'\n",
    "celeba_data_dir = '/Users/rajjain/Desktop/CourseWork/CelebA/'\n",
    "use_gpu = False\n",
    "num_cpus = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62536c6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10943069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init, Linear, Sequential, Conv2d, PReLU, Flatten, Unflatten, Module, BatchNorm2d, \\\n",
    "    ConvTranspose2d, Hardsigmoid\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchvision.datasets import CelebA\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision import transforms\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "import torch\n",
    "import numpy\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91479e",
   "metadata": {},
   "source": [
    "# VAE for dSprites dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85bfc7d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1129223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dCNNEncoderLayer(Module):\n",
    "    \"\"\"One layer of the encoder\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel, conv_stride, conv_padding: object = 0, normalise=True):\n",
    "        super(dCNNEncoderLayer, self).__init__()\n",
    "        self.conv = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(conv_kernel, conv_kernel),\n",
    "                           stride=(conv_stride, conv_stride), padding=conv_padding)\n",
    "        self.activation = PReLU(num_parameters=out_channels, init=0.25)\n",
    "        self.norm = BatchNorm2d(num_features=out_channels)\n",
    "        self.normalise = normalise\n",
    "\n",
    "    def initialise(self):\n",
    "        init.kaiming_normal_(self.conv.weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        if self.normalise:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class dCNNDecoderLayer(Module):\n",
    "    \"\"\"One layer of the decoder\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel, conv_stride, op_padding=0, normalise=True):\n",
    "        super(dCNNDecoderLayer, self).__init__()\n",
    "        self.conv_trans = ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=(conv_kernel, conv_kernel), stride=(conv_stride, conv_stride),\n",
    "                                          output_padding=op_padding)\n",
    "        self.activation = PReLU(num_parameters=out_channels, init=0.25)\n",
    "        self.normalise = normalise\n",
    "        if self.normalise:\n",
    "            self.norm = BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "    def initialise(self):\n",
    "        init.kaiming_normal_(self.conv_trans.weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_trans(x)\n",
    "        x = self.activation(x)\n",
    "        if self.normalise:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class dCNNEncoder(Module):\n",
    "    \"\"\"Encoder\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(dCNNEncoder, self).__init__()\n",
    "        self.layer1 = dCNNEncoderLayer(in_channels=1, out_channels=4, conv_kernel=3, conv_stride=1)\n",
    "        self.layer2 = dCNNEncoderLayer(in_channels=4, out_channels=8, conv_kernel=3, conv_stride=1)\n",
    "        self.layer3 = dCNNEncoderLayer(in_channels=8, out_channels=10, conv_kernel=4, conv_stride=2)\n",
    "        self.layer4 = dCNNEncoderLayer(in_channels=10, out_channels=12, conv_kernel=4, conv_stride=2)\n",
    "        self.layer5 = dCNNEncoderLayer(in_channels=12, out_channels=16, conv_kernel=4, conv_stride=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.mean = Sequential(\n",
    "            Linear(in_features=400, out_features=latent_dim),\n",
    "            PReLU(num_parameters=latent_dim, init=0.25),  # negative and positive values\n",
    "        )\n",
    "        self.logvar = Sequential(  # As suggested in original VAE paper - instead of dealing with zeros to log\n",
    "            Linear(in_features=400, out_features=latent_dim),\n",
    "            PReLU(num_parameters=latent_dim, init=0.25),  # negative and positive values\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        self.layer1.initialise()\n",
    "        self.layer2.initialise()\n",
    "        self.layer3.initialise()\n",
    "        self.layer4.initialise()\n",
    "        self.layer5.initialise()\n",
    "        init.kaiming_normal_(self.mean._modules['0'].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.kaiming_normal_(self.logvar._modules['0'].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.flatten(x)\n",
    "        mean, logvar = self.mean(x), self.logvar(x)\n",
    "        logvar = torch.clamp(logvar, min=-16, max=16)\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "class dCNNDecoder(Module):\n",
    "    \"\"\"Decoder\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(dCNNDecoder, self).__init__()\n",
    "        self.expand = Sequential(\n",
    "            Linear(in_features=latent_dim, out_features=400),\n",
    "            PReLU(num_parameters=400, init=0.25),\n",
    "        )\n",
    "        self.unflatten = Unflatten(1, (16, 5, 5))\n",
    "        self.layer1 = dCNNDecoderLayer(in_channels=16, out_channels=12, conv_kernel=4, conv_stride=2, op_padding=1)\n",
    "        self.layer2 = dCNNDecoderLayer(in_channels=12, out_channels=10, conv_kernel=4, conv_stride=2, op_padding=1)\n",
    "        self.layer3 = dCNNDecoderLayer(in_channels=10, out_channels=8, conv_kernel=4, conv_stride=2)\n",
    "        self.layer4 = dCNNDecoderLayer(in_channels=8, out_channels=4, conv_kernel=3, conv_stride=1)\n",
    "        self.output = ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.output_activation = Hardsigmoid()\n",
    "\n",
    "    def initialise(self):\n",
    "        init.kaiming_normal_(self.expand._modules['0'].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        self.layer1.initialise()\n",
    "        self.layer2.initialise()\n",
    "        self.layer3.initialise()\n",
    "        self.layer4.initialise()\n",
    "        init.xavier_normal_(self.output.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expand(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.output(x)\n",
    "        x = self.output_activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class dSprites_VAE(LightningModule):\n",
    "    \"\"\"Overall VAE Model\"\"\"\n",
    "    def __init__(self, latent_dim, num_z, reduce_kl):\n",
    "        super(dSprites_VAE, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_z = num_z  # todo: use num_z\n",
    "        self.reduce_kl = reduce_kl\n",
    "\n",
    "        self.encoder = dCNNEncoder(latent_dim)\n",
    "        self.decoder = dCNNDecoder(latent_dim)\n",
    "\n",
    "        # Initialisations\n",
    "        seed_everything(0)\n",
    "        self.encoder.initialise()\n",
    "        self.decoder.initialise()\n",
    "\n",
    "        self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get mean and std (in the form of log of variance) from the encoder\n",
    "        mean, logvar = self.encoder(x)\n",
    "\n",
    "        # Get latent sample - We'll get one sample for now - as done in the original paper\n",
    "        epsilon = torch.normal(0, 1, size=mean.shape, device=self.device)\n",
    "        z = mean + 0.5 * torch.exp(logvar) * epsilon  \n",
    "\n",
    "        # Get output from decoder\n",
    "        x_hat = self.decoder(z)\n",
    "        return mean, logvar, x_hat\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        x, = batch\n",
    "        x = x.float()\n",
    "        mean, logvar, x_hat = self(x)\n",
    "\n",
    "        # mean across samples of the L2 norm of the different vector\n",
    "        indiv_recon = MSELoss(reduction='none')(input=x_hat, target=x)\n",
    "        reconstruction_loss = 0.5 * indiv_recon.sum(dim=[1, 2, 3]).mean()\n",
    "        if self.reduce_kl:\n",
    "            # mean of KL loss across samples\n",
    "            kl_loss = 0.5 * (mean ** 2 + torch.exp(logvar) - logvar - 1).sum(dim=1).mean()  \n",
    "        else:\n",
    "            kl_loss = 0\n",
    "\n",
    "        overall_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        if torch.isnan(overall_loss).item() or torch.isinf(overall_loss).item():\n",
    "            numpy.savez_compressed(project_dir + 'issue_values', indiv_recon=indiv_recon.detach().cpu().numpy(),\n",
    "                                   mean=mean.detach().cpu().numpy(), logvar=logvar.detach().cpu().numpy(),\n",
    "                                   x=x.cpu().numpy(), x_hat=x_hat.detach().cpu().numpy())\n",
    "            raise Exception('Nan/Inf encountered')\n",
    "\n",
    "        # Log the losses\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/loss', overall_loss,              on_step=False, on_epoch=True, prog_bar=True,  \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/recon_loss', reconstruction_loss, on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/kl_loss', kl_loss,                on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "\n",
    "        return overall_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d119204",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6906f803",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                  Input Shape               Output Shape              Param #\n",
      "=============================================================================================================================\n",
      "dSprites_VAE (dSprites_VAE)                        [10, 1, 64, 64]           [10, 64]                  --\n",
      "├─dCNNEncoder (encoder): 1-1                       [10, 1, 64, 64]           [10, 64]                  --\n",
      "│    └─dCNNEncoderLayer (layer1): 2-1              [10, 1, 64, 64]           [10, 4, 62, 62]           --\n",
      "│    │    └─Conv2d (conv): 3-1                     [10, 1, 64, 64]           [10, 4, 62, 62]           40\n",
      "│    │    └─PReLU (activation): 3-2                [10, 4, 62, 62]           [10, 4, 62, 62]           4\n",
      "│    │    └─BatchNorm2d (norm): 3-3                [10, 4, 62, 62]           [10, 4, 62, 62]           8\n",
      "│    └─dCNNEncoderLayer (layer2): 2-2              [10, 4, 62, 62]           [10, 8, 60, 60]           --\n",
      "│    │    └─Conv2d (conv): 3-4                     [10, 4, 62, 62]           [10, 8, 60, 60]           296\n",
      "│    │    └─PReLU (activation): 3-5                [10, 8, 60, 60]           [10, 8, 60, 60]           8\n",
      "│    │    └─BatchNorm2d (norm): 3-6                [10, 8, 60, 60]           [10, 8, 60, 60]           16\n",
      "│    └─dCNNEncoderLayer (layer3): 2-3              [10, 8, 60, 60]           [10, 10, 29, 29]          --\n",
      "│    │    └─Conv2d (conv): 3-7                     [10, 8, 60, 60]           [10, 10, 29, 29]          1,290\n",
      "│    │    └─PReLU (activation): 3-8                [10, 10, 29, 29]          [10, 10, 29, 29]          10\n",
      "│    │    └─BatchNorm2d (norm): 3-9                [10, 10, 29, 29]          [10, 10, 29, 29]          20\n",
      "│    └─dCNNEncoderLayer (layer4): 2-4              [10, 10, 29, 29]          [10, 12, 13, 13]          --\n",
      "│    │    └─Conv2d (conv): 3-10                    [10, 10, 29, 29]          [10, 12, 13, 13]          1,932\n",
      "│    │    └─PReLU (activation): 3-11               [10, 12, 13, 13]          [10, 12, 13, 13]          12\n",
      "│    │    └─BatchNorm2d (norm): 3-12               [10, 12, 13, 13]          [10, 12, 13, 13]          24\n",
      "│    └─dCNNEncoderLayer (layer5): 2-5              [10, 12, 13, 13]          [10, 16, 5, 5]            --\n",
      "│    │    └─Conv2d (conv): 3-13                    [10, 12, 13, 13]          [10, 16, 5, 5]            3,088\n",
      "│    │    └─PReLU (activation): 3-14               [10, 16, 5, 5]            [10, 16, 5, 5]            16\n",
      "│    │    └─BatchNorm2d (norm): 3-15               [10, 16, 5, 5]            [10, 16, 5, 5]            32\n",
      "│    └─Flatten (flatten): 2-6                      [10, 16, 5, 5]            [10, 400]                 --\n",
      "│    └─Sequential (mean): 2-7                      [10, 400]                 [10, 64]                  --\n",
      "│    │    └─Linear (0): 3-16                       [10, 400]                 [10, 64]                  25,664\n",
      "│    │    └─PReLU (1): 3-17                        [10, 64]                  [10, 64]                  64\n",
      "│    └─Sequential (logvar): 2-8                    [10, 400]                 [10, 64]                  --\n",
      "│    │    └─Linear (0): 3-18                       [10, 400]                 [10, 64]                  25,664\n",
      "│    │    └─PReLU (1): 3-19                        [10, 64]                  [10, 64]                  64\n",
      "├─dCNNDecoder (decoder): 1-2                       [10, 64]                  [10, 1, 64, 64]           --\n",
      "│    └─Sequential (expand): 2-9                    [10, 64]                  [10, 400]                 --\n",
      "│    │    └─Linear (0): 3-20                       [10, 64]                  [10, 400]                 26,000\n",
      "│    │    └─PReLU (1): 3-21                        [10, 400]                 [10, 400]                 400\n",
      "│    └─Unflatten (unflatten): 2-10                 [10, 400]                 [10, 16, 5, 5]            --\n",
      "│    └─dCNNDecoderLayer (layer1): 2-11             [10, 16, 5, 5]            [10, 12, 13, 13]          --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-22     [10, 16, 5, 5]            [10, 12, 13, 13]          3,084\n",
      "│    │    └─PReLU (activation): 3-23               [10, 12, 13, 13]          [10, 12, 13, 13]          12\n",
      "│    │    └─BatchNorm2d (norm): 3-24               [10, 12, 13, 13]          [10, 12, 13, 13]          24\n",
      "│    └─dCNNDecoderLayer (layer2): 2-12             [10, 12, 13, 13]          [10, 10, 29, 29]          --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-25     [10, 12, 13, 13]          [10, 10, 29, 29]          1,930\n",
      "│    │    └─PReLU (activation): 3-26               [10, 10, 29, 29]          [10, 10, 29, 29]          10\n",
      "│    │    └─BatchNorm2d (norm): 3-27               [10, 10, 29, 29]          [10, 10, 29, 29]          20\n",
      "│    └─dCNNDecoderLayer (layer3): 2-13             [10, 10, 29, 29]          [10, 8, 60, 60]           --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-28     [10, 10, 29, 29]          [10, 8, 60, 60]           1,288\n",
      "│    │    └─PReLU (activation): 3-29               [10, 8, 60, 60]           [10, 8, 60, 60]           8\n",
      "│    │    └─BatchNorm2d (norm): 3-30               [10, 8, 60, 60]           [10, 8, 60, 60]           16\n",
      "│    └─dCNNDecoderLayer (layer4): 2-14             [10, 8, 60, 60]           [10, 4, 62, 62]           --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-31     [10, 8, 60, 60]           [10, 4, 62, 62]           292\n",
      "│    │    └─PReLU (activation): 3-32               [10, 4, 62, 62]           [10, 4, 62, 62]           4\n",
      "│    │    └─BatchNorm2d (norm): 3-33               [10, 4, 62, 62]           [10, 4, 62, 62]           8\n",
      "│    └─ConvTranspose2d (output): 2-15              [10, 4, 62, 62]           [10, 1, 64, 64]           37\n",
      "│    └─Hardsigmoid (output_activation): 2-16       [10, 1, 64, 64]           [10, 1, 64, 64]           --\n",
      "=============================================================================================================================\n",
      "Total params: 91,385\n",
      "Trainable params: 91,385\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 108.41\n",
      "=============================================================================================================================\n",
      "Input size (MB): 0.16\n",
      "Forward/backward pass size (MB): 26.72\n",
      "Params size (MB): 0.37\n",
      "Estimated Total Size (MB): 27.25\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_string = str(summary(model=dSprites_VAE(latent_dim=64, num_z=1, reduce_kl=True),\n",
    "                             input_size=[(10, 1, 64, 64)],\n",
    "                             dtypes=[torch.float],\n",
    "                             depth=3,\n",
    "                             col_names=['input_size', 'output_size', 'num_params'],\n",
    "                             row_settings=['depth', 'var_names'],\n",
    "                             verbose=0,\n",
    "                             device=torch.device('cpu')))\n",
    "print(summary_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a846d09",
   "metadata": {},
   "source": [
    "## Data Related Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc9bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dsprites_split():\n",
    "    \"\"\"Split dSprites dataset into train, val and test\"\"\"\n",
    "    num_val = num_test = 1000\n",
    "    seed_everything(0)\n",
    "\n",
    "    all_data = numpy.load(data_dir + 'dsprites.npz')['imgs']\n",
    "    all_data = numpy.expand_dims(all_data, axis=1)\n",
    "    numpy.random.shuffle(all_data)\n",
    "\n",
    "    num_samples = all_data.shape[0]\n",
    "    train, val, test = all_data[:num_samples - num_test - num_val], \\\n",
    "                       all_data[num_samples - num_test - num_val: num_samples - num_test], \\\n",
    "                       all_data[num_samples - num_test:]\n",
    "\n",
    "    numpy.savez_compressed(data_dir + 'dsprites_split.npz', train=train, val=val, test=test)\n",
    "\n",
    "\n",
    "def get_dsprites():  # use the above function to create the split\n",
    "    data = numpy.load(data_dir + 'dsprites_split.npz')\n",
    "    train, val, test = data['train'], data['val'], data['test']\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def get_dataloader(data: numpy.ndarray, bs: int, shuffle: bool):\n",
    "    data_tensor = torch.tensor(data)\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "    return DataLoader(dataset, batch_size=bs, shuffle=shuffle, num_workers=num_cpus)\n",
    "\n",
    "\n",
    "def get_dsprites_dataloaders(train, val, test, bs):\n",
    "    \"\"\"Get the dSprites dataloaders\"\"\"\n",
    "    train_dataloader = get_dataloader(train, bs, shuffle=True)\n",
    "    val_dataloader = get_dataloader(val, bs=val.shape[0], shuffle=False)\n",
    "    test_dataloader = get_dataloader(test, bs=test.shape[0], shuffle=False)\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa2f8f",
   "metadata": {},
   "source": [
    "# VAE for CelebA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c8a0b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9744799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cCNNEncoderLayer(Module):\n",
    "    \"\"\"One layer of the encoder\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel, conv_stride, conv_padding: object = 0, normalise=True):\n",
    "        super(cCNNEncoderLayer, self).__init__()\n",
    "        self.conv = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(conv_kernel, conv_kernel),\n",
    "                           stride=(conv_stride, conv_stride), padding=conv_padding)\n",
    "        self.activation = PReLU(num_parameters=out_channels, init=0.25)\n",
    "        self.norm = BatchNorm2d(num_features=out_channels)\n",
    "        self.normalise = normalise\n",
    "\n",
    "    def initialise(self):\n",
    "        init.kaiming_normal_(self.conv.weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        if self.normalise:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class cCNNDecoderLayer(Module):\n",
    "    \"\"\"One layer of the decoder\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel, conv_stride, conv_padding: object = 0, normalise=True):\n",
    "        super(cCNNDecoderLayer, self).__init__()\n",
    "        self.conv_trans = ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=(conv_kernel, conv_kernel), stride=(conv_stride, conv_stride),\n",
    "                                          padding=conv_padding)\n",
    "        self.activation = PReLU(num_parameters=out_channels, init=0.25)\n",
    "        self.normalise = normalise\n",
    "        if self.normalise:\n",
    "            self.norm = BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "    def initialise(self):\n",
    "        init.kaiming_normal_(self.conv_trans.weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_trans(x)\n",
    "        x = self.activation(x)\n",
    "        if self.normalise:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class cCNNEncoder(Module):\n",
    "    \"\"\"Encoder\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(cCNNEncoder, self).__init__()\n",
    "        self.layer1 = cCNNEncoderLayer(in_channels=3, out_channels=6, conv_kernel=3, conv_stride=1)\n",
    "        self.layer2 = cCNNEncoderLayer(in_channels=6, out_channels=8, conv_kernel=3, conv_stride=1)\n",
    "        self.layer3 = cCNNEncoderLayer(in_channels=8, out_channels=10, conv_kernel=4, conv_stride=2)\n",
    "        self.layer4 = cCNNEncoderLayer(in_channels=10, out_channels=12, conv_kernel=4, conv_stride=2)\n",
    "        self.layer5 = cCNNEncoderLayer(in_channels=12, out_channels=12, conv_kernel=6, conv_stride=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.mean = Sequential(\n",
    "            Linear(in_features=5472, out_features=latent_dim),\n",
    "            PReLU(num_parameters=latent_dim, init=0.25),  # negative and positive values\n",
    "        )\n",
    "        self.logvar = Sequential(  # As suggested in original VAE paper - instead of dealing with zeros to log\n",
    "            Linear(in_features=5472, out_features=latent_dim),\n",
    "            PReLU(num_parameters=latent_dim, init=0.25),  # negative and positive values\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        self.layer1.initialise()\n",
    "        self.layer2.initialise()\n",
    "        self.layer3.initialise()\n",
    "        self.layer4.initialise()\n",
    "        self.layer5.initialise()\n",
    "        init.kaiming_normal_(self.mean._modules['0'].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.kaiming_normal_(self.logvar._modules['0'].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.flatten(x)\n",
    "        mean, logvar = self.mean(x), self.logvar(x)\n",
    "        logvar = torch.clamp(logvar, min=-16, max=16)\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "class cCNNDecoder(Module):\n",
    "    \"\"\"Decoder\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(cCNNDecoder, self).__init__()\n",
    "        self.expand = Sequential(\n",
    "            Linear(in_features=latent_dim, out_features=5472),\n",
    "            PReLU(num_parameters=5472, init=0.25),\n",
    "        )\n",
    "        self.unflatten = Unflatten(1, (12, 24, 19))\n",
    "        self.layer1 = cCNNDecoderLayer(in_channels=12, out_channels=12, conv_kernel=6, conv_stride=2)\n",
    "        self.layer2 = cCNNDecoderLayer(in_channels=12, out_channels=10, conv_kernel=4, conv_stride=2)\n",
    "        self.layer3 = cCNNDecoderLayer(in_channels=10, out_channels=8, conv_kernel=4, conv_stride=2)\n",
    "        self.layer4 = cCNNDecoderLayer(in_channels=8, out_channels=6, conv_kernel=3, conv_stride=1)\n",
    "        self.output = ConvTranspose2d(in_channels=6, out_channels=3, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.output_activation = Hardsigmoid()\n",
    "\n",
    "    def initialise(self):\n",
    "        init.kaiming_normal_(self.expand._modules['0'].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        self.layer1.initialise()\n",
    "        self.layer2.initialise()\n",
    "        self.layer3.initialise()\n",
    "        self.layer4.initialise()\n",
    "        init.xavier_normal_(self.output.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expand(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.output(x)\n",
    "        x = self.output_activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class celeba_VAE(LightningModule):\n",
    "    \"\"\"Overall VAE Model\"\"\"\n",
    "    def __init__(self, latent_dim, num_z, reduce_kl):\n",
    "        super(celeba_VAE, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_z = num_z  # todo: use num_z\n",
    "        self.reduce_kl = reduce_kl\n",
    "\n",
    "        self.encoder = cCNNEncoder(latent_dim)\n",
    "        self.decoder = cCNNDecoder(latent_dim)\n",
    "\n",
    "        # Initialisations\n",
    "        seed_everything(0)\n",
    "        self.encoder.initialise()\n",
    "        self.decoder.initialise()\n",
    "\n",
    "        self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get mean and std (in the form of log of variance) from the encoder\n",
    "        mean, logvar = self.encoder(x)\n",
    "\n",
    "        # Get latent sample - We'll get one sample for now - as done in the original paper\n",
    "        epsilon = torch.normal(0, 1, size=mean.shape, device=self.device)\n",
    "        z = mean + 0.5 * torch.exp(logvar) * epsilon\n",
    "\n",
    "        # Get output from decoder\n",
    "        x_hat = self.decoder(z)\n",
    "        return mean, logvar, x_hat\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        x, = batch\n",
    "        x = x.float()\n",
    "        mean, logvar, x_hat = self(x)\n",
    "\n",
    "        # Mean across samples of the L2 norm of the difference vector\n",
    "        indiv_recon = MSELoss(reduction='none')(input=x_hat, target=x)\n",
    "        reconstruction_loss = 0.5 * indiv_recon.sum(dim=[1, 2, 3]).mean()\n",
    "        if self.reduce_kl:\n",
    "            # mean of KL loss across samples\n",
    "            kl_loss = 0.5 * (mean ** 2 + torch.exp(logvar) - logvar - 1).sum(dim=1).mean()  \n",
    "        else:\n",
    "            kl_loss = 0\n",
    "\n",
    "        overall_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        if torch.isnan(overall_loss).item() or torch.isinf(overall_loss).item():\n",
    "            numpy.savez_compressed(project_dir + 'issue_values', indiv_recon=indiv_recon.detach().cpu().numpy(),\n",
    "                                   mean=mean.detach().cpu().numpy(), logvar=logvar.detach().cpu().numpy(),\n",
    "                                   x=x.cpu().numpy(), x_hat=x_hat.detach().cpu().numpy())\n",
    "            raise Exception('Nan/Inf encountered')\n",
    "\n",
    "        # Log the losses\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/loss', overall_loss,              on_step=False, on_epoch=True, prog_bar=True,  \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/recon_loss', reconstruction_loss, on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/kl_loss', kl_loss,                on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "\n",
    "        return overall_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class celeba_VAE_MAE(LightningModule):\n",
    "    \"\"\"MAE as loss instead of MSE\"\"\"\n",
    "    def __init__(self, latent_dim, num_z, reduce_kl):\n",
    "        super(celeba_VAE_MAE, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_z = num_z  # todo: use num_z\n",
    "        self.reduce_kl = reduce_kl\n",
    "\n",
    "        self.encoder = cCNNEncoder(latent_dim)\n",
    "        self.decoder = cCNNDecoder(latent_dim)\n",
    "\n",
    "        # Initialisations\n",
    "        seed_everything(0)\n",
    "        self.encoder.initialise()\n",
    "        self.decoder.initialise()\n",
    "\n",
    "        self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get mean and std (in the form of log of variance) from the encoder\n",
    "        mean, logvar = self.encoder(x)\n",
    "\n",
    "        # Get latent sample - We'll get one sample for now - as done in the original paper\n",
    "        epsilon = torch.normal(0, 1, size=mean.shape, device=self.device)\n",
    "        z = mean + 0.5 * torch.exp(logvar) * epsilon\n",
    "\n",
    "        # Get output from decoder\n",
    "        x_hat = self.decoder(z)\n",
    "        return mean, logvar, x_hat\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        x, = batch\n",
    "        x = x.float()\n",
    "        mean, logvar, x_hat = self(x)\n",
    "\n",
    "        # Mean across samples of the L1 norm of the difference vector\n",
    "        indiv_recon = L1Loss(reduction='none')(input=x_hat, target=x)\n",
    "        reconstruction_loss = 0.5 * indiv_recon.sum(dim=[1, 2, 3]).mean()\n",
    "        if self.reduce_kl:\n",
    "            # mean of KL loss across samples\n",
    "            kl_loss = 0.5 * (mean ** 2 + torch.exp(logvar) - logvar - 1).sum(dim=1).mean()\n",
    "        else:\n",
    "            kl_loss = 0\n",
    "\n",
    "        overall_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        if torch.isnan(overall_loss).item() or torch.isinf(overall_loss).item():\n",
    "            numpy.savez_compressed(project_dir + 'issue_values', indiv_recon=indiv_recon.detach().cpu().numpy(),\n",
    "                                   mean=mean.detach().cpu().numpy(), logvar=logvar.detach().cpu().numpy(),\n",
    "                                   x=x.cpu().numpy(), x_hat=x_hat.detach().cpu().numpy())\n",
    "            raise Exception('Nan/Inf encountered')\n",
    "\n",
    "        # Log the losses\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/loss', overall_loss,              on_step=False, on_epoch=True, prog_bar=True,  \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/recon_loss', reconstruction_loss, on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/kl_loss', kl_loss,                on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "\n",
    "        return overall_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class celeba_VAE_meanmse(LightningModule):\n",
    "    \"\"\"MSE as loss but averaged across dimensions and across samples\"\"\"\n",
    "    def __init__(self, latent_dim, num_z, reduce_kl):\n",
    "        super(celeba_VAE_meanmse, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_z = num_z  # todo: use num_z\n",
    "        self.reduce_kl = reduce_kl\n",
    "\n",
    "        self.encoder = cCNNEncoder(latent_dim)\n",
    "        self.decoder = cCNNDecoder(latent_dim)\n",
    "\n",
    "        # Initialisations\n",
    "        seed_everything(0)\n",
    "        self.encoder.initialise()\n",
    "        self.decoder.initialise()\n",
    "\n",
    "        self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get mean and std (in the form of log of variance) from the encoder\n",
    "        mean, logvar = self.encoder(x)\n",
    "\n",
    "        # Get latent sample - We'll get one sample for now - as done in the original paper\n",
    "        epsilon = torch.normal(0, 1, size=mean.shape, device=self.device)\n",
    "        z = mean + 0.5 * torch.exp(logvar) * epsilon\n",
    "\n",
    "        # Get output from decoder\n",
    "        x_hat = self.decoder(z)\n",
    "        return mean, logvar, x_hat\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        x, = batch\n",
    "        x = x.float()\n",
    "        mean, logvar, x_hat = self(x)\n",
    "\n",
    "        # Mean across samples and dimensions of the L2 norm of the difference vector\n",
    "        indiv_recon = MSELoss(reduction='none')(input=x_hat, target=x)\n",
    "        reconstruction_loss = 0.5 * indiv_recon.mean()\n",
    "        if self.reduce_kl:\n",
    "            # mean of KL loss across samples\n",
    "            kl_loss = 0.5 * (mean ** 2 + torch.exp(logvar) - logvar - 1).sum(dim=1).mean()  \n",
    "        else:\n",
    "            kl_loss = 0\n",
    "\n",
    "        overall_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        if torch.isnan(overall_loss).item() or torch.isinf(overall_loss).item():\n",
    "            numpy.savez_compressed(project_dir + 'issue_values', indiv_recon=indiv_recon.detach().cpu().numpy(),\n",
    "                                   mean=mean.detach().cpu().numpy(), logvar=logvar.detach().cpu().numpy(),\n",
    "                                   x=x.cpu().numpy(), x_hat=x_hat.detach().cpu().numpy())\n",
    "            raise Exception('Nan/Inf encountered')\n",
    "\n",
    "        # Log the losses\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/loss', overall_loss,              on_step=False, on_epoch=True, prog_bar=True,  \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/recon_loss', reconstruction_loss, on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/kl_loss', kl_loss,                on_step=False, on_epoch=True, prog_bar=False, \n",
    "                 logger=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "\n",
    "        return overall_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29536df5",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7211e2fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                  Input Shape               Output Shape              Param #\n",
      "=============================================================================================================================\n",
      "celeba_VAE (celeba_VAE)                            [10, 3, 218, 178]         [10, 64]                  --\n",
      "├─cCNNEncoder (encoder): 1-1                       [10, 3, 218, 178]         [10, 64]                  --\n",
      "│    └─cCNNEncoderLayer (layer1): 2-1              [10, 3, 218, 178]         [10, 6, 216, 176]         --\n",
      "│    │    └─Conv2d (conv): 3-1                     [10, 3, 218, 178]         [10, 6, 216, 176]         168\n",
      "│    │    └─PReLU (activation): 3-2                [10, 6, 216, 176]         [10, 6, 216, 176]         6\n",
      "│    │    └─BatchNorm2d (norm): 3-3                [10, 6, 216, 176]         [10, 6, 216, 176]         12\n",
      "│    └─cCNNEncoderLayer (layer2): 2-2              [10, 6, 216, 176]         [10, 8, 214, 174]         --\n",
      "│    │    └─Conv2d (conv): 3-4                     [10, 6, 216, 176]         [10, 8, 214, 174]         440\n",
      "│    │    └─PReLU (activation): 3-5                [10, 8, 214, 174]         [10, 8, 214, 174]         8\n",
      "│    │    └─BatchNorm2d (norm): 3-6                [10, 8, 214, 174]         [10, 8, 214, 174]         16\n",
      "│    └─cCNNEncoderLayer (layer3): 2-3              [10, 8, 214, 174]         [10, 10, 106, 86]         --\n",
      "│    │    └─Conv2d (conv): 3-7                     [10, 8, 214, 174]         [10, 10, 106, 86]         1,290\n",
      "│    │    └─PReLU (activation): 3-8                [10, 10, 106, 86]         [10, 10, 106, 86]         10\n",
      "│    │    └─BatchNorm2d (norm): 3-9                [10, 10, 106, 86]         [10, 10, 106, 86]         20\n",
      "│    └─cCNNEncoderLayer (layer4): 2-4              [10, 10, 106, 86]         [10, 12, 52, 42]          --\n",
      "│    │    └─Conv2d (conv): 3-10                    [10, 10, 106, 86]         [10, 12, 52, 42]          1,932\n",
      "│    │    └─PReLU (activation): 3-11               [10, 12, 52, 42]          [10, 12, 52, 42]          12\n",
      "│    │    └─BatchNorm2d (norm): 3-12               [10, 12, 52, 42]          [10, 12, 52, 42]          24\n",
      "│    └─cCNNEncoderLayer (layer5): 2-5              [10, 12, 52, 42]          [10, 12, 24, 19]          --\n",
      "│    │    └─Conv2d (conv): 3-13                    [10, 12, 52, 42]          [10, 12, 24, 19]          5,196\n",
      "│    │    └─PReLU (activation): 3-14               [10, 12, 24, 19]          [10, 12, 24, 19]          12\n",
      "│    │    └─BatchNorm2d (norm): 3-15               [10, 12, 24, 19]          [10, 12, 24, 19]          24\n",
      "│    └─Flatten (flatten): 2-6                      [10, 12, 24, 19]          [10, 5472]                --\n",
      "│    └─Sequential (mean): 2-7                      [10, 5472]                [10, 64]                  --\n",
      "│    │    └─Linear (0): 3-16                       [10, 5472]                [10, 64]                  350,272\n",
      "│    │    └─PReLU (1): 3-17                        [10, 64]                  [10, 64]                  64\n",
      "│    └─Sequential (logvar): 2-8                    [10, 5472]                [10, 64]                  --\n",
      "│    │    └─Linear (0): 3-18                       [10, 5472]                [10, 64]                  350,272\n",
      "│    │    └─PReLU (1): 3-19                        [10, 64]                  [10, 64]                  64\n",
      "├─cCNNDecoder (decoder): 1-2                       [10, 64]                  [10, 3, 218, 178]         --\n",
      "│    └─Sequential (expand): 2-9                    [10, 64]                  [10, 5472]                --\n",
      "│    │    └─Linear (0): 3-20                       [10, 64]                  [10, 5472]                355,680\n",
      "│    │    └─PReLU (1): 3-21                        [10, 5472]                [10, 5472]                5,472\n",
      "│    └─Unflatten (unflatten): 2-10                 [10, 5472]                [10, 12, 24, 19]          --\n",
      "│    └─cCNNDecoderLayer (layer1): 2-11             [10, 12, 24, 19]          [10, 12, 52, 42]          --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-22     [10, 12, 24, 19]          [10, 12, 52, 42]          5,196\n",
      "│    │    └─PReLU (activation): 3-23               [10, 12, 52, 42]          [10, 12, 52, 42]          12\n",
      "│    │    └─BatchNorm2d (norm): 3-24               [10, 12, 52, 42]          [10, 12, 52, 42]          24\n",
      "│    └─cCNNDecoderLayer (layer2): 2-12             [10, 12, 52, 42]          [10, 10, 106, 86]         --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-25     [10, 12, 52, 42]          [10, 10, 106, 86]         1,930\n",
      "│    │    └─PReLU (activation): 3-26               [10, 10, 106, 86]         [10, 10, 106, 86]         10\n",
      "│    │    └─BatchNorm2d (norm): 3-27               [10, 10, 106, 86]         [10, 10, 106, 86]         20\n",
      "│    └─cCNNDecoderLayer (layer3): 2-13             [10, 10, 106, 86]         [10, 8, 214, 174]         --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-28     [10, 10, 106, 86]         [10, 8, 214, 174]         1,288\n",
      "│    │    └─PReLU (activation): 3-29               [10, 8, 214, 174]         [10, 8, 214, 174]         8\n",
      "│    │    └─BatchNorm2d (norm): 3-30               [10, 8, 214, 174]         [10, 8, 214, 174]         16\n",
      "│    └─cCNNDecoderLayer (layer4): 2-14             [10, 8, 214, 174]         [10, 6, 216, 176]         --\n",
      "│    │    └─ConvTranspose2d (conv_trans): 3-31     [10, 8, 214, 174]         [10, 6, 216, 176]         438\n",
      "│    │    └─PReLU (activation): 3-32               [10, 6, 216, 176]         [10, 6, 216, 176]         6\n",
      "│    │    └─BatchNorm2d (norm): 3-33               [10, 6, 216, 176]         [10, 6, 216, 176]         12\n",
      "│    └─ConvTranspose2d (output): 2-15              [10, 6, 216, 176]         [10, 3, 218, 178]         165\n",
      "│    └─Hardsigmoid (output_activation): 2-16       [10, 3, 218, 178]         [10, 3, 218, 178]         --\n",
      "=============================================================================================================================\n",
      "Total params: 1,080,119\n",
      "Trainable params: 1,080,119\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.42\n",
      "=============================================================================================================================\n",
      "Input size (MB): 4.66\n",
      "Forward/backward pass size (MB): 320.33\n",
      "Params size (MB): 4.32\n",
      "Estimated Total Size (MB): 329.31\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_string = str(summary(model=celeba_VAE(latent_dim=64, num_z=1, reduce_kl=True),\n",
    "                             input_size=[(10, 3, 218, 178)],\n",
    "                             dtypes=[torch.float],\n",
    "                             depth=3,\n",
    "                             col_names=['input_size', 'output_size', 'num_params'],\n",
    "                             row_settings=['depth', 'var_names'],\n",
    "                             verbose=0,\n",
    "                             device=torch.device('cpu')))\n",
    "print(summary_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98f3b4",
   "metadata": {},
   "source": [
    "## Data Related Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9bb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celeba_dataloaders(train, val, test, bs):\n",
    "    \"\"\"Setting num_workers = 0 as some issue with jupyter and pytorch. in normal implementation, num_cpus is used. \n",
    "    Check GitHub code\"\"\"\n",
    "    \n",
    "    def custom_collate_fn(batch):\n",
    "        imgs = torch.stack([elem[0] for elem in batch])\n",
    "        return [imgs]\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        CelebA(celeba_data_dir, split=train, target_type=[], transform=transforms.Compose([transforms.ToTensor()])),\n",
    "        bs, shuffle=True, num_workers=0, collate_fn=custom_collate_fn\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        CelebA(celeba_data_dir, split=val, target_type=[], transform=transforms.Compose([transforms.ToTensor()])),\n",
    "        bs, shuffle=False, num_workers=0, collate_fn=custom_collate_fn\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        CelebA(celeba_data_dir, split=test, target_type=[], transform=transforms.Compose([transforms.ToTensor()])),\n",
    "        bs, shuffle=False, num_workers=0, collate_fn=custom_collate_fn\n",
    "    )\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537bc3e",
   "metadata": {},
   "source": [
    "# Common Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d71d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(bs: int, max_epochs: int,\n",
    "                   tags: list[str], gpu_num: list[int], interactive: bool,\n",
    "                   model_class, model_kwargs: dict,\n",
    "                   loss_desc: str,\n",
    "                   train, val, test, get_dataloaders, input_shape):\n",
    "    seed_everything(0)\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = get_dataloaders(train, val, test, bs)\n",
    "\n",
    "    folder_name = datetime.utcnow().isoformat(sep=\"T\", timespec=\"microseconds\")\n",
    "    results_dir = project_dir + f'vae/results/run_{folder_name}/'\n",
    "    os.makedirs(results_dir, exist_ok=False)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor='val/loss', mode='min', patience=100)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val/loss', mode='min', dirpath=results_dir, filename='best')\n",
    "\n",
    "    if use_gpu:\n",
    "        trainer_kwargs = dict(accelerator=\"gpu\", devices=gpu_num,\n",
    "                              strategy=None if interactive else DDPStrategy(find_unused_parameters=False))\n",
    "    else:\n",
    "        trainer_kwargs = dict()\n",
    "\n",
    "    tf_logger = TensorBoardLogger(save_dir=results_dir, version=f'tf_logs',\n",
    "                                  default_hp_metric=False)\n",
    "\n",
    "    model = model_class(**model_kwargs)\n",
    "\n",
    "    trainer = Trainer(auto_lr_find=True, default_root_dir=results_dir, max_epochs=max_epochs,\n",
    "                      callbacks=[early_stop_callback, checkpoint_callback], logger=[tf_logger],\n",
    "                      log_every_n_steps=1, num_sanity_val_steps=0,  \n",
    "                      limit_train_batches=4, limit_val_batches=1, limit_test_batches=1,\n",
    "                      **trainer_kwargs)\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "    model = model_class.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "    if use_gpu:\n",
    "        trainer_kwargs = dict(accelerator=\"gpu\", devices=gpu_num[:1],\n",
    "                              strategy=None if interactive else DDPStrategy(find_unused_parameters=False))\n",
    "    else:\n",
    "        trainer_kwargs = dict()\n",
    "    trainer = Trainer(auto_lr_find=True, default_root_dir=results_dir, max_epochs=max_epochs,\n",
    "                      callbacks=[early_stop_callback, checkpoint_callback], logger=[tf_logger],\n",
    "                      log_every_n_steps=1, num_sanity_val_steps=0,  \n",
    "                      limit_train_batches=4, limit_val_batches=1, limit_test_batches=1,\n",
    "                      **trainer_kwargs)\n",
    "    trainer.test(model, dataloaders=test_dataloader)\n",
    "\n",
    "    summary_string = str(summary(model=model, input_size=[(10, *input_shape)],\n",
    "                                 dtypes=[torch.float], depth=3, verbose=0,\n",
    "                                 col_names=['input_size', 'output_size', 'num_params'],\n",
    "                                 row_settings=['depth', 'var_names'], device=torch.device('cpu'))) + '\\n' + loss_desc\n",
    "    with open(results_dir + 'model_desc.md', 'w') as f:\n",
    "        f.write(summary_string)\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bbc51",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31251227",
   "metadata": {},
   "source": [
    "## dSprites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8eaeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "\n",
      "  | Name    | Type        | Params\n",
      "----------------------------------------\n",
      "0 | encoder | dCNNEncoder | 32.5 K\n",
      "1 | decoder | dCNNDecoder | 20.3 K\n",
      "----------------------------------------\n",
      "52.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "52.9 K    Total params\n",
      "0.211     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023910045623779297,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fc964750e4417e912a744fc83097a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007883071899414062,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0077419281005859375,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00812220573425293,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6348a00511d45de8aadc4f6bc127ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test/kl_loss          0.9098517298698425\n",
      "        test/loss            442.3057556152344\n",
      "     test/recon_loss         441.3959045410156\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "train, val, test = get_dsprites()\n",
    "train_and_test(bs=256, max_epochs=2, tags=[], gpu_num=[], interactive=True,\n",
    "               model_class=dSprites_VAE, model_kwargs=dict(latent_dim=32, num_z=1, reduce_kl=True),\n",
    "               loss_desc='Recon & KL Loss, Mean across samples',\n",
    "               train=train, val=val, test=test, get_dataloaders=get_dsprites_dataloaders,\n",
    "               input_shape=(1, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4fac4",
   "metadata": {},
   "source": [
    "## CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "318c8060",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "\n",
      "  | Name    | Type        | Params\n",
      "----------------------------------------\n",
      "0 | encoder | cCNNEncoder | 2.8 M \n",
      "1 | decoder | cCNNDecoder | 1.4 M \n",
      "----------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.931    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008494138717651367,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955245fc620849e0aa01eaeb2ba6b378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026761293411254883,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014943838119506836,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009268999099731445,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 17,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6683ec594cb7416db687c82e82813f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test/kl_loss          14.146760940551758\n",
      "        test/loss            5421.04736328125\n",
      "     test/recon_loss          5406.900390625\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "train_and_test(bs=256, max_epochs=2, tags=[], gpu_num=[], interactive=True,\n",
    "               model_class=celeba_VAE, model_kwargs=dict(latent_dim=256, num_z=1, reduce_kl=True),\n",
    "               loss_desc='Recon & KL Loss, Mean across samples',\n",
    "               train='train', val='valid', test='test', get_dataloaders=get_celeba_dataloaders, \n",
    "               input_shape=(3, 218, 178))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f95961",
   "metadata": {},
   "source": [
    "# Plots & Analysis\n",
    "\n",
    "See GitHub code for examples of usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da76f80",
   "metadata": {},
   "source": [
    "## dSprites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "833a5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disentanglement_results(result_dir):\n",
    "    model = dSprites_VAE.load_from_checkpoint(project_dir + 'vae/dsprites/results/' + result_dir + '/best.ckpt')\n",
    "\n",
    "    zs = []\n",
    "    for dimi in numpy.linspace(-2, 2, 10):\n",
    "        for dimj in numpy.linspace(-2, 2, 10):\n",
    "            zs.append([dimi, dimj] + [0] * (model.latent_dim - 2))\n",
    "\n",
    "    z = torch.tensor(numpy.asarray(zs), dtype=torch.float)\n",
    "\n",
    "    decoded = model.decoder(z)\n",
    "    x_hat = decoded.detach().numpy().squeeze(axis=1)\n",
    "\n",
    "    fig = make_subplots(rows=10, cols=10, shared_xaxes=True, shared_yaxes=True,\n",
    "                        horizontal_spacing=0.001, vertical_spacing=0.001)\n",
    "\n",
    "    for i in range(x_hat.shape[0]):\n",
    "        col = i % 10 + 1\n",
    "        row = int(i / 10) + 1\n",
    "        fig.add_heatmap(z=1-x_hat[i, :, :], row=row, col=col, colorscale='Greys', showscale=False)\n",
    "\n",
    "    if model.reduce_kl:\n",
    "        title = f'dSprites - Disentanglement Effect - Latent Dim = {model.latent_dim}'\n",
    "        img_name = f'dim={model.latent_dim}.png'\n",
    "    else:\n",
    "        title = f'dSprites - Disentanglement Effect - Latent Dim = {model.latent_dim}, KL Loss not minimised'\n",
    "        img_name = f'dim={model.latent_dim}_kl_notminimised.png'\n",
    "\n",
    "    fig.update_layout(width=1200, height=1200, title=title, title_x=0.5, title_font_size=20, title_xref='paper',\n",
    "                      title_y=0.94)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.write_image(project_dir + f'vae/dsprites/img_results/vae_dsprites_disent_{img_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_some_generations(result_dir):\n",
    "    model = dSprites_VAE.load_from_checkpoint(project_dir + 'vae/dsprites/results/' + result_dir + '/best.ckpt')\n",
    "    num_imgs = 100\n",
    "    z = torch.normal(0, 1, size=(num_imgs, model.latent_dim))\n",
    "    decoded = model.decoder(z)\n",
    "    x_hat = decoded.detach().numpy().squeeze(axis=1)\n",
    "\n",
    "    fig = make_subplots(rows=10, cols=10, shared_xaxes=True, shared_yaxes=True,\n",
    "                        horizontal_spacing=0.001, vertical_spacing=0.001)\n",
    "\n",
    "    for i in range(x_hat.shape[0]):\n",
    "        col = i % 10 + 1\n",
    "        row = int(i / 10) + 1\n",
    "        fig.add_heatmap(z=1-x_hat[i, :, :], row=row, col=col, colorscale='Greys', showscale=False)\n",
    "\n",
    "    if model.reduce_kl:\n",
    "        title = f'dSprites - Generated Images - Latent Dim = {model.latent_dim}'\n",
    "        img_name = f'dim={model.latent_dim}.png'\n",
    "    else:\n",
    "        title = f'dSprites - Generated Images - Latent Dim = {model.latent_dim}, KL Loss not minimised'\n",
    "        img_name = f'dim={model.latent_dim}_kl_notminimised.png'\n",
    "\n",
    "    fig.update_layout(width=1200, height=1200, title=title, title_x=0.5, title_font_size=20, title_xref='paper',\n",
    "                      title_y=0.94)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.write_image(project_dir + f'vae/dsprites/img_results/vae_dsprites_gen_{img_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_some_reconstructions(result_dir):\n",
    "    model = dSprites_VAE.load_from_checkpoint(project_dir + 'vae/dsprites/results/' + result_dir + '/best.ckpt')\n",
    "\n",
    "    targets = numpy.load(data_dir + 'dsprites_split.npz')['test']\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.float)\n",
    "    _, _, predictions = model(targets_tensor)\n",
    "    predictions = predictions.detach().numpy()\n",
    "    \n",
    "    assert targets.shape == predictions.shape\n",
    "    indexes = numpy.random.choice(targets.shape[0], (50, ))\n",
    "    actual = numpy.squeeze(targets[indexes], axis=1)\n",
    "    pred = numpy.squeeze(predictions[indexes], axis=1)\n",
    "\n",
    "    fig = make_subplots(rows=10, cols=10, shared_xaxes=True, shared_yaxes=True,\n",
    "                        horizontal_spacing=0.001, vertical_spacing=0.001)\n",
    "\n",
    "    for i in range(indexes.shape[0]):\n",
    "        tcol = (i % 5) + 1\n",
    "        tcol = 2 * tcol - 1\n",
    "        pcol = tcol + 1\n",
    "        row = int(i / 5) + 1\n",
    "        fig.add_heatmap(z=1-actual[i, :, :], row=row, col=tcol, colorscale='Greys', showscale=False)\n",
    "        fig.add_heatmap(z=1-pred[i, :, :], row=row, col=pcol, colorscale='Greys', showscale=False)\n",
    "\n",
    "    if model.reduce_kl:\n",
    "        title = f'dSprites - Reconstructed Images - Latent Dim = {model.latent_dim}'\n",
    "        img_name = f'dim={model.latent_dim}.png'\n",
    "    else:\n",
    "        title = f'dSprites - Reconstructed Images - Latent Dim = {model.latent_dim}, KL Loss not minimised'\n",
    "        img_name = f'dim={model.latent_dim}_kl_notminimised.png'\n",
    "\n",
    "    fig.update_layout(width=1200, height=1200, title=title, title_x=0.5, title_font_size=20, title_xref='paper',\n",
    "                      title_y=0.94)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.write_image(project_dir + f'vae/dsprites/img_results/vae_dsprites_recon_{img_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000863",
   "metadata": {},
   "source": [
    "## CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a26e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disentanglement_results(result_dir: str, extra_config: str):\n",
    "    model = celeba_VAE.load_from_checkpoint(project_dir + 'vae/celeba/results/' + result_dir + '/best.ckpt')\n",
    "\n",
    "    zs = []\n",
    "    for dimi in numpy.linspace(-2, 2, 10):\n",
    "        for dimj in numpy.linspace(-2, 2, 10):\n",
    "            zs.append([dimi, dimj] + [0] * (model.latent_dim - 2))\n",
    "\n",
    "    z = torch.tensor(numpy.asarray(zs), dtype=torch.float)\n",
    "\n",
    "    decoded = model.decoder(z)\n",
    "    x_hat = decoded.detach().numpy()\n",
    "    x_hat = numpy.transpose(x_hat, (0, 2, 3, 1))\n",
    "\n",
    "    fig, axes = plt.subplots(10, 10, sharex='all', sharey='all', figsize=(8, 8))\n",
    "    fig.subplots_adjust(wspace=0.01, hspace=0.01, left=0, bottom=0, right=1, top=0.95)\n",
    "    axes = axes.flat\n",
    "\n",
    "    for i in range(x_hat.shape[0]):\n",
    "        ax = axes[i]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(x_hat[i])\n",
    "\n",
    "    if model.reduce_kl:\n",
    "        if extra_config:\n",
    "            title = f'CelebA - Disentanglement Effect - Latent Dim = {model.latent_dim} with {extra_config}'\n",
    "            img_name = f'dim={model.latent_dim}_extra={extra_config.replace(\" \", \"\")}.png'\n",
    "        else:\n",
    "            title = f'CelebA - Disentanglement Effect - Latent Dim = {model.latent_dim}'\n",
    "            img_name = f'dim={model.latent_dim}.png'\n",
    "    else:\n",
    "        title = f'CelebA - Disentanglement Effect - Latent Dim = {model.latent_dim}, KL Loss not minimised'\n",
    "        img_name = f'dim={model.latent_dim}_kl_notminimised.png'\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig(project_dir + f'vae/celeba/img_results/vae_celeba_disent_{img_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05f75a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_some_generations(result_dir: str, extra_config: str):\n",
    "    model = celeba_VAE.load_from_checkpoint(project_dir + 'vae/celeba/results/' + result_dir + '/best.ckpt')\n",
    "    num_imgs = 100\n",
    "    z = torch.normal(0, 1, size=(num_imgs, model.latent_dim))\n",
    "    decoded = model.decoder(z)\n",
    "    x_hat = decoded.detach().numpy()\n",
    "    x_hat = numpy.transpose(x_hat, (0, 2, 3, 1))\n",
    "\n",
    "    fig, axes = plt.subplots(10, 10, sharex='all', sharey='all', figsize=(8, 8))\n",
    "    fig.subplots_adjust(wspace=0.01, hspace=0.01, left=0, bottom=0, right=1, top=0.95)\n",
    "    axes = axes.flat\n",
    "\n",
    "    for i in range(x_hat.shape[0]):\n",
    "        ax = axes[i]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(x_hat[i])\n",
    "\n",
    "    if model.reduce_kl:\n",
    "        if extra_config:\n",
    "            title = f'CelebA - Generated Images - Latent Dim = {model.latent_dim} with {extra_config}'\n",
    "            img_name = f'dim={model.latent_dim}_extra={extra_config.replace(\" \", \"\")}.png'\n",
    "        else:\n",
    "            title = f'CelebA - Generated Images - Latent Dim = {model.latent_dim}'\n",
    "            img_name = f'dim={model.latent_dim}.png'\n",
    "    else:\n",
    "        title = f'CelebA - Generated Images - Latent Dim = {model.latent_dim}, KL Loss not minimised'\n",
    "        img_name = f'dim={model.latent_dim}_kl_notminimised.png'\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig(project_dir + f'vae/celeba/img_results/vae_celeba_gen_{img_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af09b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_some_reconstructions(result_dir: str, targets, extra_config: str):\n",
    "    model = celeba_VAE.load_from_checkpoint(project_dir + 'vae/celeba/results/' + result_dir + '/best.ckpt')\n",
    "    _, _, predictions = model(targets)\n",
    "    predictions = predictions.detach().numpy()\n",
    "\n",
    "    assert targets.shape == predictions.shape\n",
    "    predictions = numpy.transpose(predictions, (0, 2, 3, 1))\n",
    "    targets = numpy.transpose(targets, (0, 2, 3, 1))\n",
    "\n",
    "    fig, axes = plt.subplots(10, 10, sharex='all', sharey='all', figsize=(8, 8))\n",
    "    fig.subplots_adjust(wspace=0.01, hspace=0.01, left=0, bottom=0, right=1, top=0.95)\n",
    "    axes = axes.flat\n",
    "\n",
    "    for i in range(targets.shape[0]):\n",
    "        target_idx = 2 * i\n",
    "        ax = axes[target_idx]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(targets[i])\n",
    "\n",
    "        pred_idx = target_idx + 1\n",
    "        ax = axes[pred_idx]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(predictions[i])\n",
    "\n",
    "    if model.reduce_kl:\n",
    "        if extra_config:\n",
    "            title = f'CelebA - Reconstructed Images - Latent Dim = {model.latent_dim} with {extra_config}'\n",
    "            img_name = f'dim={model.latent_dim}_extra={extra_config.replace(\" \", \"\")}.png'\n",
    "        else:\n",
    "            title = f'CelebA - Reconstructed Images - Latent Dim = {model.latent_dim}'\n",
    "            img_name = f'dim={model.latent_dim}.png'\n",
    "    else:\n",
    "        title = f'CelebA - Reconstructed Images - Latent Dim = {model.latent_dim}, KL Loss not minimised'\n",
    "        img_name = f'dim={model.latent_dim}_kl_notminimised.png'\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig(project_dir + f'vae/celeba/img_results/vae_celeba_recon_{img_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66767420",
   "metadata": {},
   "source": [
    "## Marginal Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34a6b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_evaluated_at(model, x: torch.Tensor, z: torch.Tensor):\n",
    "    \"\"\"Evaluate Gradient at one z\"\"\"\n",
    "    z.requires_grad_()\n",
    "    x_hat = model.decoder(z)\n",
    "    x_hat = torch.squeeze(x_hat, dim=0)\n",
    "    mse_loss = 0.5 * MSELoss(reduction='sum')(input=x_hat, target=x)\n",
    "    mse_loss.backward()\n",
    "    gradient = -z - z.grad\n",
    "    gradient.detach_()\n",
    "    z.detach_()\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def take_one_step(model, x: torch.Tensor, old_z: torch.Tensor, step_size: int = 0.001):\n",
    "    \"\"\"Make one gradient ascent step\"\"\"\n",
    "    gradient = gradient_evaluated_at(model, x, old_z)\n",
    "    new_z = old_z + step_size * gradient\n",
    "    return new_z\n",
    "\n",
    "\n",
    "def get_samples_from_posterior(model, x: torch.Tensor, ignore: int = 20, L: int = 50) -> numpy.ndarray:\n",
    "    \"\"\"Get L samples from the MCMC\"\"\"\n",
    "    z = torch.normal(0, 1, size=(1, model.latent_dim))\n",
    "    for _ in range(ignore):  # ignore first few steps\n",
    "        z = take_one_step(model, x, z)\n",
    "\n",
    "    zs = []\n",
    "    for _ in range(L):  # start recording the samples\n",
    "        z = take_one_step(model, x, z)\n",
    "        zs.append(z)\n",
    "\n",
    "    zs = torch.concat(zs).numpy()\n",
    "    return zs\n",
    "\n",
    "\n",
    "def get_q(model, x: torch.Tensor, apply_pca=True):\n",
    "    \"\"\"Use GMMs as density estimator. Use PCA on data if mentioned. Use the learned PCA going forward\"\"\"\n",
    "    zs = get_samples_from_posterior(model, x)\n",
    "    gmm = GaussianMixture(n_components=4, init_params='k-means++')\n",
    "    if apply_pca:\n",
    "        pca = PCA(n_components=4)\n",
    "        zs = pca.fit_transform(zs)\n",
    "    else:\n",
    "        pca = None\n",
    "    gmm.fit(zs)\n",
    "    return gmm, pca\n",
    "\n",
    "\n",
    "def evaluate_prior(model, z):\n",
    "    \"\"\"Evaluate the log prior\"\"\"\n",
    "    normalise = -model.latent_dim * numpy.log(2 * math.pi) / 2\n",
    "    log_prob = -z.dot(z) / 2\n",
    "    return normalise + log_prob\n",
    "\n",
    "\n",
    "def evaluate_posterior(model, x, z):\n",
    "    \"\"\"Evaluate the log posterior\"\"\"\n",
    "    z = torch.tensor(z).reshape(1, -1)\n",
    "    x_hat = model.decoder(z)\n",
    "    x_hat = torch.squeeze(x_hat, dim=0)\n",
    "    diff = x - x_hat\n",
    "    diff = diff.detach().numpy().reshape(-1)\n",
    "    normalise = -model.latent_dim * numpy.log(2 * math.pi) / 2\n",
    "    log_prob = -diff.dot(diff) / 2\n",
    "    return normalise + log_prob\n",
    "\n",
    "\n",
    "def log_likelihood_estimate(model, x: torch.Tensor):\n",
    "    \"\"\"Get the likelihood estimate for one sample, x\"\"\"\n",
    "    gmm, pca = get_q(model, x, apply_pca=model.latent_dim > 5)  # Get the density estimator. \n",
    "                                                                # Apply PCA if latent dim is more than 5\n",
    "\n",
    "    zs = get_samples_from_posterior(model, x)  # Get fresh samples from MCMC \n",
    "    if pca:\n",
    "        tzs = pca.transform(zs)\n",
    "    else:\n",
    "        tzs = zs\n",
    "\n",
    "    values = []\n",
    "    for j in range(zs.shape[0]):\n",
    "        z = zs[j]\n",
    "        tz = tzs[j]\n",
    "        log_q = gmm.score_samples(tz.reshape(1, -1))[0]  # Get the density estimate \n",
    "        log_prior = evaluate_prior(model, z)\n",
    "        log_posterior = evaluate_posterior(model, x, z)\n",
    "        value = log_q - log_prior - log_posterior  # Get all values in terms of log\n",
    "        val = numpy.exp(value)  # Get actual value\n",
    "        values.append(val)\n",
    "    inv = numpy.mean(values)  # Take the mean\n",
    "    ll = -numpy.log(inv)  # Get the log likelihood estimate \n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cf17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
