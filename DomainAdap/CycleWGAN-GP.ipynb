{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8648350",
   "metadata": {},
   "source": [
    "### General Settings\n",
    "\n",
    "Change the respective settings to run appropriately\n",
    "\n",
    "Use `limit_train_batches`, `limit_val_batches`, `limit_test_batches` as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb53b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/Users/rajjain/PycharmProjects/ADRL-Course-Work/'\n",
    "data_dir = project_dir + 'data/'\n",
    "mnist_data_dir = '/Users/rajjain/Desktop/CourseWork/MNIST/'\n",
    "usps_data_dir = '/Users/rajjain/Desktop/CourseWork/USPS/'\n",
    "use_gpu = False\n",
    "num_cpus = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf986c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init, Linear, Sequential, Conv2d, PReLU, Module, BatchNorm2d, ConvTranspose2d, Hardtanh, \\\n",
    "    Flatten, L1Loss, BatchNorm1d, Unflatten, CrossEntropyLoss\n",
    "from pytorch_lightning.trainer.supporters import CombinedLoader\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchvision.datasets import USPS, MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torchinfo import summary\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import shutil\n",
    "import torch\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0284c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    imgs = torch.stack([elem[0] for elem in batch])\n",
    "    return [imgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360eee7a",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4dcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U2MGenerator(Module):\n",
    "    \"\"\"\n",
    "    Take USPS image [-1, 1] and generate \"fake\" MNIST image [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(U2MGenerator, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            # Downsampling part\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "            BatchNorm2d(num_features=8),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=200),\n",
    "            PReLU(num_parameters=200, init=0.25),\n",
    "            BatchNorm1d(num_features=200),\n",
    "\n",
    "            Unflatten(1, (8, 5, 5)),\n",
    "\n",
    "            # Upsampling part\n",
    "            ConvTranspose2d(in_channels=8, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            ConvTranspose2d(in_channels=6, out_channels=3, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            Hardtanh(min_val=-1, max_val=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [0, 3, 6, 10, 14, 17]:\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model._modules['20'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class M2UGenerator(Module):\n",
    "    \"\"\"\n",
    "    Take MNIST image [-1, 1] and generate \"fake\" USPS image [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(M2UGenerator, self).__init__()\n",
    "\n",
    "        self.model = Sequential(\n",
    "            # Downsampling part\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "            BatchNorm2d(num_features=8),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=200),\n",
    "            PReLU(num_parameters=200, init=0.25),\n",
    "            BatchNorm1d(num_features=200),\n",
    "\n",
    "            Unflatten(1, (8, 5, 5)),\n",
    "\n",
    "            # Upsampling part\n",
    "            ConvTranspose2d(in_channels=8, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            ConvTranspose2d(in_channels=6, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            Hardtanh(min_val=-1, max_val=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [0, 3, 6, 10, 14, 17]:\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model._modules['20'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CriticM(Module):\n",
    "    \"\"\"\n",
    "    Critic for MNIST images - Checks if the mnist image passed is from mnist \"real\" data distribution or from \"fake\" U2M generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CriticM, self).__init__()\n",
    "\n",
    "        self.model = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [0, 2, 4]:\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model._modules['7'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CriticU(Module):\n",
    "    \"\"\"\n",
    "    Critic for USPS images - Checks if the usps image passed is from usps \"real\" data distribution or from \"fake\" M2U generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CriticU, self).__init__()\n",
    "\n",
    "        self.model = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [0, 2, 4]:\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model._modules['7'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MUCycleGANGP(LightningModule):\n",
    "\n",
    "    def __init__(self, ncritic, ngen, bs, cycle_weight, penalty_weight: float):\n",
    "        super(MUCycleGANGP, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.ncritic = ncritic\n",
    "        self.ngen = ngen\n",
    "        self.bs = bs\n",
    "        self.cycle_weight = cycle_weight\n",
    "        self.penalty_weight = penalty_weight\n",
    "\n",
    "        # USPS to MNIST\n",
    "        self.genU2M = U2MGenerator()\n",
    "        self.criticM = CriticM()\n",
    "\n",
    "        # MNIST to USPS\n",
    "        self.genM2U = M2UGenerator()\n",
    "        self.criticU = CriticU()\n",
    "\n",
    "        # Initialisations\n",
    "        self.genU2M.initialise()\n",
    "        self.criticM.initialise()\n",
    "        self.genM2U.initialise()\n",
    "        self.criticU.initialise()\n",
    "\n",
    "        # CycleGAN authors use image pool to update the discriminator. That was required because GAN training (on the\n",
    "        # usual objective) was unstable. We are using WGAN and hopefully won't get into such issues. Hence, skipping\n",
    "        # keeping the pool of images\n",
    "\n",
    "        self.float()\n",
    "\n",
    "    def _get_gradient_penalty(self, critic, reals, fakes):\n",
    "        batch_size = reals.shape[0]\n",
    "        eps = torch.rand(batch_size, 1, 1, 1, device=self.device)\n",
    "        eps = eps.expand_as(reals)\n",
    "        interpolated = eps * reals + (1 - eps) * fakes\n",
    "        interpolated.requires_grad_(True)\n",
    "        interpolated_scores = critic(interpolated)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolated_scores,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(interpolated_scores),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradients_norm = gradients.norm(2, 1)  # norm of gradient each of the samples\n",
    "        penalty = (gradients_norm - 1) ** 2  # penalty for each sample\n",
    "        return self.penalty_weight * penalty.mean()  # mean across samples\n",
    "\n",
    "    def _critic_losses(self, real_uspss, real_mnists, btype):\n",
    "        # WGAN: Critic gets updated from the fake and real data\n",
    "        # CycleGAN: Need to do this for both critics!\n",
    "\n",
    "        # MNIST Critic\n",
    "        real_mnists_score = self.criticM(real_mnists).mean()\n",
    "        fake_mnists = self.genU2M(real_uspss)\n",
    "        fake_mnists_score = self.criticM(fake_mnists).mean()\n",
    "        criticM_loss = fake_mnists_score - real_mnists_score  # minimise this!\n",
    "\n",
    "        criticM_gp = self._get_gradient_penalty(self.criticM, real_mnists, fake_mnists)\n",
    "\n",
    "        # USPS Critic\n",
    "        real_uspss_score = self.criticU(real_uspss).mean()\n",
    "        fake_uspss = self.genM2U(real_mnists)\n",
    "        fake_uspss_score = self.criticU(fake_uspss).mean()\n",
    "        criticU_loss = fake_uspss_score - real_uspss_score\n",
    "\n",
    "        criticU_gp = self._get_gradient_penalty(self.criticU, real_uspss, fake_uspss)\n",
    "\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/criticM_loss', criticM_loss, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/criticU_loss', criticU_loss, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/criticM_gp', criticM_gp, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/criticU_gp', criticU_gp, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "\n",
    "        self.log(f'{btype}/real_mnists_score', real_mnists_score, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/fake_mnists_score', fake_mnists_score, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/real_uspss_score', real_uspss_score, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/fake_uspss_score', fake_uspss_score, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        return criticM_loss, criticU_loss, criticM_gp, criticU_gp\n",
    "\n",
    "    def _generator_losses(self, real_uspss, real_mnists, btype):\n",
    "        # WGAN: Generator gets updates from the fake data\n",
    "        # CycleGAN: Do this for both generators and additionally put cycle consistency loss\n",
    "\n",
    "        # USPS to MNIST\n",
    "        fake_mnists = self.genU2M(real_uspss)\n",
    "        fake_mnists_score = self.criticM(fake_mnists).mean()\n",
    "        genU2M_loss = -fake_mnists_score  # minimise this!\n",
    "\n",
    "        # MNIST to USPS\n",
    "        fake_uspss = self.genM2U(real_mnists)\n",
    "        fake_uspss_score = self.criticU(fake_uspss).mean()\n",
    "        genM2U_loss = -fake_uspss_score\n",
    "\n",
    "        # Cycle Consistency\n",
    "        # Side Note: *Ideally* L1 norm should be added across dimensions and mean-ed across samples. In their\n",
    "        # implementation, authors have mean-ed across dimensions too, keeping same implementation as them\n",
    "        usps_identity_loss = L1Loss()(real_uspss, self.genM2U(fake_mnists))\n",
    "        mnist_identity_loss = L1Loss()(real_mnists, self.genU2M(fake_uspss))\n",
    "        cycle_loss = self.cycle_weight * (usps_identity_loss + mnist_identity_loss)\n",
    "\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/genU2M_loss', genU2M_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/genM2U_loss', genM2U_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/cycle_loss', cycle_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/usps_identity_loss', usps_identity_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/mnist_identity_loss', mnist_identity_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "\n",
    "        return genU2M_loss, genM2U_loss, cycle_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real_uspss, real_mnists = batch['usps'][0], batch['mnist'][0]\n",
    "\n",
    "        if optimizer_idx == 1:  # Critic optimizer - only update Critic weights\n",
    "            criticM_loss, criticU_loss, criticM_gp, criticU_gp = self._critic_losses(real_uspss, real_mnists, 'train')\n",
    "            # CycleGAN authors divide this loss by 2 to slow down the rate of critic learning. Here, as the loss is\n",
    "            # wasserstein loss, I believe it may not be required\n",
    "            return criticM_loss + criticU_loss + criticM_gp + criticU_gp\n",
    "\n",
    "        if optimizer_idx == 0:  # Generator optimizer - only update Generator weights\n",
    "            genU2M_loss, genM2U_loss, cycle_loss = self._generator_losses(real_uspss, real_mnists, 'train')\n",
    "            return genU2M_loss + genM2U_loss + cycle_loss\n",
    "\n",
    "        # Is there a way to consolidate the losses and return one per epoch?\n",
    "        raise Exception(f'Unknown optimizer index: {optimizer_idx}')\n",
    "\n",
    "    def _shared_eval(self, batch, btype):\n",
    "        real_uspss, real_mnists = batch['usps'][0], batch['mnist'][0]\n",
    "        criticM_loss, criticU_loss, _, _ = self._critic_losses(real_uspss, real_mnists, btype)\n",
    "        _, _, cycle_loss = self._generator_losses(real_uspss, real_mnists, btype)\n",
    "\n",
    "        mnist_emd = -criticM_loss\n",
    "        usps_emd = -criticU_loss\n",
    "        overall_loss = mnist_emd + usps_emd + cycle_loss\n",
    "\n",
    "        self.log(f'{btype}/mnist_emd', mnist_emd, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=True)\n",
    "        self.log(f'{btype}/usps_emd', usps_emd, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=True)\n",
    "        self.log(f'{btype}/overall_loss', overall_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        torch.set_grad_enabled(True)\n",
    "        self._shared_eval(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        torch.set_grad_enabled(True)\n",
    "        self._shared_eval(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Using the strategy from WGAN paper instead of CycleGAN paper!\"\"\"\n",
    "        generator_opt = Adam(params=itertools.chain(self.genU2M.parameters(), self.genM2U.parameters()), lr=0.0001, betas=(0, 0.9))\n",
    "        critic_opt = Adam(params=itertools.chain(self.criticU.parameters(), self.criticM.parameters()), lr=0.0001, betas=(0, 0.9))\n",
    "        return (\n",
    "            {\"optimizer\": generator_opt, \"frequency\": self.ngen},\n",
    "            {\"optimizer\": critic_opt, \"frequency\": self.ncritic},\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=True, num_workers=0,\n",
    "                                     collate_fn=custom_collate_fn, drop_last=True)\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=True, num_workers=0,\n",
    "                                      collate_fn=custom_collate_fn, drop_last=True)\n",
    "        return {\n",
    "            'usps': usps_dataloader,\n",
    "            'mnist': mnist_dataloader,\n",
    "        }\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=False,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=False, num_workers=0,\n",
    "                                     collate_fn=custom_collate_fn, drop_last=True)\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=False, num_workers=0,\n",
    "                                      collate_fn=custom_collate_fn, drop_last=True)\n",
    "        return CombinedLoader({\n",
    "            'usps': usps_dataloader,\n",
    "            'mnist': mnist_dataloader,\n",
    "        }, mode='max_size_cycle')\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=False,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=False, num_workers=0,\n",
    "                                     collate_fn=custom_collate_fn, drop_last=True)\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=False, num_workers=0,\n",
    "                                      collate_fn=custom_collate_fn, drop_last=True)\n",
    "        return CombinedLoader({\n",
    "            'usps': usps_dataloader,\n",
    "            'mnist': mnist_dataloader,\n",
    "        }, mode='max_size_cycle')\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        _summary_kwargs = dict(dtypes=[torch.float], depth=3, col_names=['input_size', 'output_size', 'num_params'],\n",
    "                               row_settings=['depth', 'var_names'], verbose=0, device=torch.device('cpu'))\n",
    "        _usps = torch.randn((10, 1, 16, 16), dtype=torch.float)\n",
    "        _mnist = torch.randn((10, 1, 28, 28), dtype=torch.float)\n",
    "        _summary_string = str(summary(model=self.genU2M, input_data=_usps, **_summary_kwargs)) + '\\n' + \\\n",
    "                          str(summary(model=self.criticU, input_data=_usps, **_summary_kwargs)) + '\\n' + \\\n",
    "                          str(summary(model=self.genM2U, input_data=_mnist, **_summary_kwargs)) + '\\n' + \\\n",
    "                          str(summary(model=self.criticM, input_data=_mnist, **_summary_kwargs))\n",
    "        return _summary_string\n",
    "\n",
    "\n",
    "class USPSClassifier(LightningModule):\n",
    "    \"\"\"\n",
    "    Classify USPS image [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bs):\n",
    "        super(USPSClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bs = bs\n",
    "        self.model = Sequential(\n",
    "            # Downsampling part\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "            BatchNorm2d(num_features=8),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=10),\n",
    "        )\n",
    "        self.float()\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [0, 3, 6]:\n",
    "            init.kaiming_normal_(self.model[i].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model[10].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        not_training = btype != 'train'\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = CrossEntropyLoss()(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, average='macro', num_classes=10, multiclass=True)\n",
    "        self.log(f'{btype}/usps_classifier_loss', loss, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/usps_classifier_acc', acc, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=True, num_workers=0)\n",
    "        return usps_dataloader\n",
    "\n",
    "    def eval_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=False,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=False, num_workers=0)\n",
    "        return usps_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.eval_dataloader()\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.eval_dataloader()\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        summary_kwargs = dict(dtypes=[torch.float], depth=3, col_names=['input_size', 'output_size', 'num_params'],\n",
    "                              row_settings=['depth', 'var_names'], verbose=0, device=torch.device('cpu'))\n",
    "        usps_imgs = torch.randn((10, 1, 16, 16), dtype=torch.float)\n",
    "        summary_string = str(summary(model=self, input_data=usps_imgs, **summary_kwargs))\n",
    "        return summary_string\n",
    "\n",
    "\n",
    "class MNISTClassifier(LightningModule):\n",
    "    \"\"\"\n",
    "    Classify MNIST image [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bs):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bs = bs\n",
    "        self.model = Sequential(\n",
    "            # Downsampling part\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "            BatchNorm2d(num_features=8),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=10),\n",
    "        )\n",
    "        self.float()\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [0, 3, 6]:\n",
    "            init.kaiming_normal_(self.model[i].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model[10].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _common_step(self, batch, btype):\n",
    "        not_training = btype != 'train'\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = CrossEntropyLoss()(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, average='macro', num_classes=10, multiclass=True)\n",
    "        self.log(f'{btype}/mnist_classifier_loss', loss, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        self.log(f'{btype}/mnist_classifier_acc', acc, on_step=False, on_epoch=True, sync_dist=not_training)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=True, num_workers=0)\n",
    "        return mnist_dataloader\n",
    "\n",
    "    def eval_dataloader(self):\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=False, num_workers=0)\n",
    "        return mnist_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.eval_dataloader()\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.eval_dataloader()\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        summary_kwargs = dict(dtypes=[torch.float], depth=3, col_names=['input_size', 'output_size', 'num_params'],\n",
    "                              row_settings=['depth', 'var_names'], verbose=0, device=torch.device('cpu'))\n",
    "        usps_imgs = torch.randn((10, 1, 28, 28), dtype=torch.float)\n",
    "        summary_string = str(summary(model=self, input_data=usps_imgs, **summary_kwargs))\n",
    "        return summary_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708dbfc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "U2MGenerator (U2MGenerator)              [10, 1, 16, 16]           [10, 1, 28, 28]           --\n",
      "├─Sequential (model): 1-1                [10, 1, 16, 16]           [10, 1, 28, 28]           --\n",
      "│    └─Conv2d (0): 2-1                   [10, 1, 16, 16]           [10, 3, 14, 14]           30\n",
      "│    └─PReLU (1): 2-2                    [10, 3, 14, 14]           [10, 3, 14, 14]           3\n",
      "│    └─BatchNorm2d (2): 2-3              [10, 3, 14, 14]           [10, 3, 14, 14]           6\n",
      "│    └─Conv2d (3): 2-4                   [10, 3, 14, 14]           [10, 6, 12, 12]           168\n",
      "│    └─PReLU (4): 2-5                    [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─BatchNorm2d (5): 2-6              [10, 6, 12, 12]           [10, 6, 12, 12]           12\n",
      "│    └─Conv2d (6): 2-7                   [10, 6, 12, 12]           [10, 8, 5, 5]             776\n",
      "│    └─PReLU (7): 2-8                    [10, 8, 5, 5]             [10, 8, 5, 5]             8\n",
      "│    └─BatchNorm2d (8): 2-9              [10, 8, 5, 5]             [10, 8, 5, 5]             16\n",
      "│    └─Flatten (9): 2-10                 [10, 8, 5, 5]             [10, 200]                 --\n",
      "│    └─Linear (10): 2-11                 [10, 200]                 [10, 200]                 40,200\n",
      "│    └─PReLU (11): 2-12                  [10, 200]                 [10, 200]                 200\n",
      "│    └─BatchNorm1d (12): 2-13            [10, 200]                 [10, 200]                 400\n",
      "│    └─Unflatten (13): 2-14              [10, 200]                 [10, 8, 5, 5]             --\n",
      "│    └─ConvTranspose2d (14): 2-15        [10, 8, 5, 5]             [10, 6, 12, 12]           774\n",
      "│    └─PReLU (15): 2-16                  [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─BatchNorm2d (16): 2-17            [10, 6, 12, 12]           [10, 6, 12, 12]           12\n",
      "│    └─ConvTranspose2d (17): 2-18        [10, 6, 12, 12]           [10, 3, 26, 26]           291\n",
      "│    └─PReLU (18): 2-19                  [10, 3, 26, 26]           [10, 3, 26, 26]           3\n",
      "│    └─BatchNorm2d (19): 2-20            [10, 3, 26, 26]           [10, 3, 26, 26]           6\n",
      "│    └─ConvTranspose2d (20): 2-21        [10, 3, 26, 26]           [10, 1, 28, 28]           28\n",
      "│    └─Hardtanh (21): 2-22               [10, 1, 28, 28]           [10, 1, 28, 28]           --\n",
      "===================================================================================================================\n",
      "Total params: 42,945\n",
      "Trainable params: 42,945\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 4.20\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.20\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 1.38\n",
      "===================================================================================================================\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "CriticU (CriticU)                        [10, 1, 16, 16]           [10, 1]                   --\n",
      "├─Sequential (model): 1-1                [10, 1, 16, 16]           [10, 1]                   --\n",
      "│    └─Conv2d (0): 2-1                   [10, 1, 16, 16]           [10, 3, 14, 14]           30\n",
      "│    └─PReLU (1): 2-2                    [10, 3, 14, 14]           [10, 3, 14, 14]           3\n",
      "│    └─Conv2d (2): 2-3                   [10, 3, 14, 14]           [10, 6, 12, 12]           168\n",
      "│    └─PReLU (3): 2-4                    [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─Conv2d (4): 2-5                   [10, 6, 12, 12]           [10, 8, 5, 5]             776\n",
      "│    └─PReLU (5): 2-6                    [10, 8, 5, 5]             [10, 8, 5, 5]             8\n",
      "│    └─Flatten (6): 2-7                  [10, 8, 5, 5]             [10, 200]                 --\n",
      "│    └─Linear (7): 2-8                   [10, 200]                 [10, 1]                   201\n",
      "===================================================================================================================\n",
      "Total params: 1,192\n",
      "Trainable params: 1,192\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.50\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.26\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.28\n",
      "===================================================================================================================\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "M2UGenerator (M2UGenerator)              [10, 1, 28, 28]           [10, 1, 16, 16]           --\n",
      "├─Sequential (model): 1-1                [10, 1, 28, 28]           [10, 1, 16, 16]           --\n",
      "│    └─Conv2d (0): 2-1                   [10, 1, 28, 28]           [10, 3, 26, 26]           30\n",
      "│    └─PReLU (1): 2-2                    [10, 3, 26, 26]           [10, 3, 26, 26]           3\n",
      "│    └─BatchNorm2d (2): 2-3              [10, 3, 26, 26]           [10, 3, 26, 26]           6\n",
      "│    └─Conv2d (3): 2-4                   [10, 3, 26, 26]           [10, 6, 12, 12]           294\n",
      "│    └─PReLU (4): 2-5                    [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─BatchNorm2d (5): 2-6              [10, 6, 12, 12]           [10, 6, 12, 12]           12\n",
      "│    └─Conv2d (6): 2-7                   [10, 6, 12, 12]           [10, 8, 5, 5]             776\n",
      "│    └─PReLU (7): 2-8                    [10, 8, 5, 5]             [10, 8, 5, 5]             8\n",
      "│    └─BatchNorm2d (8): 2-9              [10, 8, 5, 5]             [10, 8, 5, 5]             16\n",
      "│    └─Flatten (9): 2-10                 [10, 8, 5, 5]             [10, 200]                 --\n",
      "│    └─Linear (10): 2-11                 [10, 200]                 [10, 200]                 40,200\n",
      "│    └─PReLU (11): 2-12                  [10, 200]                 [10, 200]                 200\n",
      "│    └─BatchNorm1d (12): 2-13            [10, 200]                 [10, 200]                 400\n",
      "│    └─Unflatten (13): 2-14              [10, 200]                 [10, 8, 5, 5]             --\n",
      "│    └─ConvTranspose2d (14): 2-15        [10, 8, 5, 5]             [10, 6, 12, 12]           774\n",
      "│    └─PReLU (15): 2-16                  [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─BatchNorm2d (16): 2-17            [10, 6, 12, 12]           [10, 6, 12, 12]           12\n",
      "│    └─ConvTranspose2d (17): 2-18        [10, 6, 12, 12]           [10, 3, 14, 14]           165\n",
      "│    └─PReLU (18): 2-19                  [10, 3, 14, 14]           [10, 3, 14, 14]           3\n",
      "│    └─BatchNorm2d (19): 2-20            [10, 3, 14, 14]           [10, 3, 14, 14]           6\n",
      "│    └─ConvTranspose2d (20): 2-21        [10, 3, 14, 14]           [10, 1, 16, 16]           28\n",
      "│    └─Hardtanh (21): 2-22               [10, 1, 16, 16]           [10, 1, 16, 16]           --\n",
      "===================================================================================================================\n",
      "Total params: 42,945\n",
      "Trainable params: 42,945\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.74\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 1.16\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 1.36\n",
      "===================================================================================================================\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "CriticM (CriticM)                        [10, 1, 28, 28]           [10, 1]                   --\n",
      "├─Sequential (model): 1-1                [10, 1, 28, 28]           [10, 1]                   --\n",
      "│    └─Conv2d (0): 2-1                   [10, 1, 28, 28]           [10, 3, 26, 26]           30\n",
      "│    └─PReLU (1): 2-2                    [10, 3, 26, 26]           [10, 3, 26, 26]           3\n",
      "│    └─Conv2d (2): 2-3                   [10, 3, 26, 26]           [10, 6, 12, 12]           294\n",
      "│    └─PReLU (3): 2-4                    [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─Conv2d (4): 2-5                   [10, 6, 12, 12]           [10, 8, 5, 5]             776\n",
      "│    └─PReLU (5): 2-6                    [10, 8, 5, 5]             [10, 8, 5, 5]             8\n",
      "│    └─Flatten (6): 2-7                  [10, 8, 5, 5]             [10, 200]                 --\n",
      "│    └─Linear (7): 2-8                   [10, 200]                 [10, 1]                   201\n",
      "===================================================================================================================\n",
      "Total params: 1,318\n",
      "Trainable params: 1,318\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.82\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.49\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.53\n",
      "===================================================================================================================\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "USPSClassifier (USPSClassifier)          [10, 1, 16, 16]           [10, 10]                  --\n",
      "├─Sequential (model): 1-1                [10, 1, 16, 16]           [10, 10]                  --\n",
      "│    └─Conv2d (0): 2-1                   [10, 1, 16, 16]           [10, 3, 14, 14]           30\n",
      "│    └─PReLU (1): 2-2                    [10, 3, 14, 14]           [10, 3, 14, 14]           3\n",
      "│    └─BatchNorm2d (2): 2-3              [10, 3, 14, 14]           [10, 3, 14, 14]           6\n",
      "│    └─Conv2d (3): 2-4                   [10, 3, 14, 14]           [10, 6, 12, 12]           168\n",
      "│    └─PReLU (4): 2-5                    [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─BatchNorm2d (5): 2-6              [10, 6, 12, 12]           [10, 6, 12, 12]           12\n",
      "│    └─Conv2d (6): 2-7                   [10, 6, 12, 12]           [10, 8, 5, 5]             776\n",
      "│    └─PReLU (7): 2-8                    [10, 8, 5, 5]             [10, 8, 5, 5]             8\n",
      "│    └─BatchNorm2d (8): 2-9              [10, 8, 5, 5]             [10, 8, 5, 5]             16\n",
      "│    └─Flatten (9): 2-10                 [10, 8, 5, 5]             [10, 200]                 --\n",
      "│    └─Linear (10): 2-11                 [10, 200]                 [10, 10]                  2,010\n",
      "===================================================================================================================\n",
      "Total params: 3,035\n",
      "Trainable params: 3,035\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.52\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.40\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.42\n",
      "===================================================================================================================\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "MNISTClassifier (MNISTClassifier)        [10, 1, 28, 28]           [10, 10]                  --\n",
      "├─Sequential (model): 1-1                [10, 1, 28, 28]           [10, 10]                  --\n",
      "│    └─Conv2d (0): 2-1                   [10, 1, 28, 28]           [10, 3, 26, 26]           30\n",
      "│    └─PReLU (1): 2-2                    [10, 3, 26, 26]           [10, 3, 26, 26]           3\n",
      "│    └─BatchNorm2d (2): 2-3              [10, 3, 26, 26]           [10, 3, 26, 26]           6\n",
      "│    └─Conv2d (3): 2-4                   [10, 3, 26, 26]           [10, 6, 12, 12]           294\n",
      "│    └─PReLU (4): 2-5                    [10, 6, 12, 12]           [10, 6, 12, 12]           6\n",
      "│    └─BatchNorm2d (5): 2-6              [10, 6, 12, 12]           [10, 6, 12, 12]           12\n",
      "│    └─Conv2d (6): 2-7                   [10, 6, 12, 12]           [10, 8, 5, 5]             776\n",
      "│    └─PReLU (7): 2-8                    [10, 8, 5, 5]             [10, 8, 5, 5]             8\n",
      "│    └─BatchNorm2d (8): 2-9              [10, 8, 5, 5]             [10, 8, 5, 5]             16\n",
      "│    └─Flatten (9): 2-10                 [10, 8, 5, 5]             [10, 200]                 --\n",
      "│    └─Linear (10): 2-11                 [10, 200]                 [10, 10]                  2,010\n",
      "===================================================================================================================\n",
      "Total params: 3,161\n",
      "Trainable params: 3,161\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.84\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.74\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.79\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(MUCycleGANGP(1, 1, 1, 1, 1).summary())\n",
    "print(USPSClassifier(1).summary())\n",
    "print(MNISTClassifier(1).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24449dbe",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80679323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(max_epochs: int, tags: list[str], gpu_num: list[int],\n",
    "                   model_class, model_kwargs: dict, model_desc: str):\n",
    "    seed_everything(0, workers=True)\n",
    "\n",
    "    folder_name = datetime.utcnow().isoformat(sep=\"T\", timespec=\"microseconds\")\n",
    "    results_dir = project_dir + f'domain_adap/cycle_gan/results/run_{folder_name}/'\n",
    "    os.makedirs(results_dir, exist_ok=False)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val/overall_loss', mode='min', dirpath=results_dir,\n",
    "                                          save_last=True, save_top_k=1, auto_insert_metric_name=False,\n",
    "                                          filename='epoch={epoch}-val_overall_loss={val/overall_loss:.4f}')\n",
    "\n",
    "    if use_gpu:\n",
    "        trainer_kwargs = dict(accelerator=\"gpu\", devices=gpu_num)\n",
    "    else:\n",
    "        trainer_kwargs = dict()\n",
    "\n",
    "    tf_logger = TensorBoardLogger(save_dir=results_dir, version=f'tf_logs',\n",
    "                                  default_hp_metric=False)\n",
    "\n",
    "    model = model_class(**model_kwargs)\n",
    "\n",
    "    trainer = Trainer(default_root_dir=results_dir, max_epochs=max_epochs,\n",
    "                      callbacks=[checkpoint_callback], logger=[tf_logger],\n",
    "                      log_every_n_steps=1, num_sanity_val_steps=0, multiple_trainloader_mode='max_size_cycle',\n",
    "                      limit_train_batches=6, limit_val_batches=6, limit_test_batches=6,\n",
    "                      deterministic=True, **trainer_kwargs)\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "\n",
    "    summary = model.summary() + '\\n' + model_desc\n",
    "    with open(results_dir + 'model_desc.md', 'w') as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    gc.collect()\n",
    "    return f'run_{folder_name}'\n",
    "\n",
    "\n",
    "def train_and_test_classifier(max_epochs: int, tags: list[str], gpu_num: list[int],\n",
    "                              model_class, model_kwargs: dict, model_desc: str, dataset: str):\n",
    "    seed_everything(0, workers=True)\n",
    "\n",
    "    folder_name = datetime.utcnow().isoformat(sep=\"T\", timespec=\"microseconds\")\n",
    "    results_dir = project_dir + f'domain_adap/cycle_gan/results/run_{folder_name}/'\n",
    "    os.makedirs(results_dir, exist_ok=False)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=f'val/{dataset}_classifier_loss', mode='min', dirpath=results_dir,\n",
    "                                          filename='best')\n",
    "\n",
    "    if use_gpu:\n",
    "        trainer_kwargs = dict(accelerator=\"gpu\", devices=gpu_num)\n",
    "    else:\n",
    "        trainer_kwargs = dict()\n",
    "\n",
    "    tf_logger = TensorBoardLogger(save_dir=results_dir, version=f'tf_logs',\n",
    "                                  default_hp_metric=False)\n",
    "\n",
    "    model = model_class(**model_kwargs)\n",
    "\n",
    "    trainer = Trainer(default_root_dir=results_dir, max_epochs=max_epochs,\n",
    "                      callbacks=[checkpoint_callback], logger=[tf_logger],\n",
    "                      log_every_n_steps=1, num_sanity_val_steps=0,\n",
    "                      limit_train_batches=20, limit_val_batches=20, limit_test_batches=20,\n",
    "                      deterministic=True, **trainer_kwargs)\n",
    "    trainer.fit(model)\n",
    "    trainer.test(ckpt_path='best')\n",
    "\n",
    "    summary = model.summary() + '\\n' + model_desc\n",
    "    with open(results_dir + 'model_desc.md', 'w') as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    gc.collect()\n",
    "    return f'run_{folder_name}'\n",
    "\n",
    "\n",
    "def test_target(test_dl, generator, classifier):\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    from torchmetrics.functional.classification import accuracy\n",
    "    ys, y_hats = [], []\n",
    "    for x, y in tqdm(test_dl):\n",
    "        gen_x = generator(x)\n",
    "        y_hat = classifier(gen_x)\n",
    "        ys.append(y)\n",
    "        y_hats.append(y_hat)\n",
    "    all_y = torch.concat(ys)\n",
    "    all_y_hat = torch.concat(y_hats)\n",
    "    loss = CrossEntropyLoss()(all_y_hat, all_y).item()\n",
    "    acc = accuracy(all_y_hat, all_y, average='macro', num_classes=10, multiclass=True).item()\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def test_target_accuracies(mnist_classifier_folder, usps_classifier_folder, cycle_gan_folder):\n",
    "    mnist_classifier = MNISTClassifier.load_from_checkpoint(project_dir + 'domain_adap/cycle_gan/results/' +\n",
    "                                                            mnist_classifier_folder + '/best.ckpt', bs=20)\n",
    "    usps_classifier = USPSClassifier.load_from_checkpoint(project_dir + 'domain_adap/cycle_gan/results/' +\n",
    "                                                          usps_classifier_folder + '/best.ckpt', bs=20)\n",
    "\n",
    "    cycle_gan_direc = project_dir + 'domain_adap/cycle_gan/results/' + cycle_gan_folder + '/'\n",
    "    best_fname = glob(cycle_gan_direc + 'epoch*.ckpt')[0]\n",
    "    cycle_gan = MUCycleGANGP.load_from_checkpoint(best_fname)\n",
    "\n",
    "    mnist_test_dl = mnist_classifier.test_dataloader()\n",
    "    usps_test_dl = usps_classifier.test_dataloader()\n",
    "\n",
    "    mnist_loss, mnist_acc = test_target(mnist_test_dl, cycle_gan.genM2U, usps_classifier)\n",
    "    usps_loss, usps_acc = test_target(usps_test_dl, cycle_gan.genU2M, mnist_classifier)\n",
    "\n",
    "    print(f'Target: MNIST -- CE Loss = {mnist_loss:2.4f}, Acc = {mnist_acc * 100:.2f}%')\n",
    "    print(f'Target:  USPS -- CE Loss = {usps_loss:2.4f}, Acc = {usps_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a968f7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 3.2 K \n",
      "-------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "/Users/rajjain/miniforge3/envs/gpfa_latents/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/rajjain/miniforge3/envs/gpfa_latents/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012953996658325195,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dd3d16f16340d4a41594bfeacb3807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007772922515869141,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007344722747802734,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/rajjain/PycharmProjects/ADRL-Course-Work/domain_adap/cycle_gan/results/run_2022-11-10T08:08:09.835223/best.ckpt\n",
      "Loaded model weights from checkpoint at /Users/rajjain/PycharmProjects/ADRL-Course-Work/domain_adap/cycle_gan/results/run_2022-11-10T08:08:09.835223/best.ckpt\n",
      "/Users/rajjain/miniforge3/envs/gpfa_latents/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007431983947753906,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732a6f3480a74d04a19d8b874f69ba0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 3.0 K \n",
      "-------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric               DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "test/mnist_classifier_acc     0.3514385223388672\n",
      "test/mnist_classifier_loss    1.8667727708816528\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007447957992553711,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da39fde11e084dd2a2755f94518e8714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007770061492919922,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007531881332397461,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/rajjain/PycharmProjects/ADRL-Course-Work/domain_adap/cycle_gan/results/run_2022-11-10T08:08:10.426572/best.ckpt\n",
      "Loaded model weights from checkpoint at /Users/rajjain/PycharmProjects/ADRL-Course-Work/domain_adap/cycle_gan/results/run_2022-11-10T08:08:10.426572/best.ckpt\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007314920425415039,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c905cf355d5d45bbb5c27c3927545110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | genU2M  | U2MGenerator | 42.9 K\n",
      "1 | criticM | CriticM      | 1.3 K \n",
      "2 | genM2U  | M2UGenerator | 42.9 K\n",
      "3 | criticU | CriticU      | 1.2 K \n",
      "-----------------------------------------\n",
      "88.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "88.4 K    Total params\n",
      "0.354     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "test/usps_classifier_acc    0.5545138716697693\n",
      "test/usps_classifier_loss    1.342002272605896\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007458925247192383,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4837cdc4c97340238628e4f4a336ea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007798194885253906,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0073049068450927734,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007445096969604492,
       "initial": 0,
       "n": 0,
       "ncols": 155,
       "nrows": 9,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9e693b97074e5a9f796464853e8a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test/criticM_gp        0.30373769998550415\n",
      "    test/criticM_loss       1.0020878314971924\n",
      "     test/criticU_gp        0.07823963463306427\n",
      "    test/criticU_loss       0.1398886740207672\n",
      "     test/cycle_loss        18.734392166137695\n",
      " test/fake_mnists_score    0.008811501786112785\n",
      "  test/fake_uspss_score     0.4236476719379425\n",
      "    test/genM2U_loss        -0.4236476719379425\n",
      "    test/genU2M_loss       -0.008811501786112785\n",
      "     test/mnist_emd         -1.0020878314971924\n",
      "test/mnist_identity_loss    1.1771318912506104\n",
      "    test/overall_loss        17.59241485595703\n",
      " test/real_mnists_score     -0.9932763576507568\n",
      "  test/real_uspss_score     0.2837590277194977\n",
      "      test/usps_emd         -0.1398886740207672\n",
      " test/usps_identity_loss     0.696307361125946\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 306.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101/101 [00:00<00:00, 252.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: MNIST -- CE Loss = 2.5223, Acc = 10.58%\n",
      "Target:  USPS -- CE Loss = 2.4421, Acc = 9.68%\n"
     ]
    }
   ],
   "source": [
    "mnist_classifier_folder = train_and_test_classifier(2, tags=[], gpu_num=[], model_class=MNISTClassifier,\n",
    "                                                    model_kwargs=dict(bs=10), model_desc='MNIST Classifier', \n",
    "                                                    dataset='mnist')\n",
    "\n",
    "usps_classifier_folder = train_and_test_classifier(2, tags=[], gpu_num=[], model_class=USPSClassifier,\n",
    "                                                   model_kwargs=dict(bs=10), model_desc='USPS Classifier', \n",
    "                                                   dataset='usps')\n",
    "\n",
    "cycle_gan_folder = train_and_test(max_epochs=2, tags=[], gpu_num=[], model_class=MUCycleGANGP,\n",
    "                                  model_kwargs=dict(ncritic=2, ngen=1, cycle_weight=10, bs=10, penalty_weight=1),\n",
    "                                  model_desc='WGAN with Gradient Penalty; EMD estimates + Cycle Loss as a overall loss')\n",
    "\n",
    "test_target_accuracies(mnist_classifier_folder, usps_classifier_folder, cycle_gan_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50fe0d",
   "metadata": {},
   "source": [
    "# Plots & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side(title, reals, gens, fname):\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "    fig.subplots_adjust(wspace=0.01, hspace=0.01, left=0, bottom=0, right=1, top=0.95)\n",
    "    axes = axes.flat\n",
    "\n",
    "    for i in range(reals.shape[0]):\n",
    "        target_idx = 2 * i\n",
    "        ax = axes[target_idx]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(reals[i], cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "        pred_idx = target_idx + 1\n",
    "        ax = axes[pred_idx]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(gens[i], cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig(project_dir + f'domain_adap/cycle_gan/img_results/{fname}.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def convert_to_image(ndarray):  # -1 to 1\n",
    "    ndarray = ndarray * 0.5 + 0.5  # 0 to 1\n",
    "    ndarray *= 255  # 0 to 255\n",
    "    ndarray = numpy.round(ndarray, decimals=0)  # rounded off\n",
    "    return ndarray.astype(int)\n",
    "\n",
    "\n",
    "def see_some_translations(model: MUCycleGANGP, mnist, usps, model_type, epochs):\n",
    "    gen_usps = model.genM2U(mnist)\n",
    "    gen_mnist = model.genU2M(usps)\n",
    "\n",
    "    id_usps = model.genM2U(gen_mnist)\n",
    "    id_mnist = model.genU2M(gen_usps)\n",
    "\n",
    "    mnist = convert_to_image(numpy.transpose(mnist.numpy(), (0, 2, 3, 1)))\n",
    "    usps = convert_to_image(numpy.transpose(usps.numpy(), (0, 2, 3, 1)))\n",
    "    gen_usps = convert_to_image(numpy.transpose(gen_usps.detach().numpy(), (0, 2, 3, 1)))\n",
    "    gen_mnist = convert_to_image(numpy.transpose(gen_mnist.detach().numpy(), (0, 2, 3, 1)))\n",
    "    id_usps = convert_to_image(numpy.transpose(id_usps.detach().numpy(), (0, 2, 3, 1)))\n",
    "    id_mnist = convert_to_image(numpy.transpose(id_mnist.detach().numpy(), (0, 2, 3, 1)))\n",
    "\n",
    "    plot_side_by_side(\n",
    "        f'GP - MNIST to USPS - NCritic = {model.ncritic}, NGen = {model.ngen}, Cycle Weight = {model.cycle_weight}, Penalty = {model.penalty_weight}, Epochs = {epochs} - {model_type}',\n",
    "        mnist, gen_usps,\n",
    "        f'gan_gp_trans_ncritic={model.ncritic}_ngen={model.ngen}_cycleweight={model.cycle_weight}_penalty={model.penalty_weight}_epochs={epochs}_{model_type}_m2u')\n",
    "    plot_side_by_side(\n",
    "        f'GP - USPS to MNIST - NCritic = {model.ncritic}, NGen = {model.ngen}, Cycle Weight = {model.cycle_weight}, Penalty = {model.penalty_weight}, Epochs = {epochs} - {model_type}',\n",
    "        usps, gen_mnist,\n",
    "        f'gan_gp_trans_ncritic={model.ncritic}_ngen={model.ngen}_cycleweight={model.cycle_weight}_penalty={model.penalty_weight}_epochs={epochs}_{model_type}_u2m')\n",
    "\n",
    "    plot_side_by_side(\n",
    "        f'GP - MNIST to MNIST - NCritic = {model.ncritic}, NGen = {model.ngen}, Cycle Weight = {model.cycle_weight}, Penalty = {model.penalty_weight}, Epochs = {epochs} - {model_type}',\n",
    "        mnist, id_mnist,\n",
    "        f'gan_gp_id_ncritic={model.ncritic}_ngen={model.ngen}_cycleweight={model.cycle_weight}_penalty={model.penalty_weight}_epochs={epochs}_{model_type}_m2m')\n",
    "    plot_side_by_side(\n",
    "        f'GP - USPS to USPS - NCritic = {model.ncritic}, NGen = {model.ngen}, Cycle Weight = {model.cycle_weight}, Penalty = {model.penalty_weight}, Epochs = {epochs} - {model_type}',\n",
    "        usps, id_usps,\n",
    "        f'gan_gp_id_ncritic={model.ncritic}_ngen={model.ngen}_cycleweight={model.cycle_weight}_penalty={model.penalty_weight}_epochs={epochs}_{model_type}_u2u')\n",
    "\n",
    "\n",
    "def get_data(num_images):\n",
    "    mnist_dataset = MNIST(mnist_data_dir, train=False,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                          ]))\n",
    "    mnist_dataloader = DataLoader(mnist_dataset, num_images, shuffle=False, num_workers=num_cpus,\n",
    "                                  collate_fn=custom_collate_fn)\n",
    "    mnist = next(iter(mnist_dataloader))\n",
    "\n",
    "    usps_dataset = USPS(usps_data_dir, train=False,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                            transforms.Normalize((0.5,), (0.5,)),\n",
    "                        ]))\n",
    "    usps_dataloader = DataLoader(usps_dataset, num_images, shuffle=False, num_workers=num_cpus,\n",
    "                                 collate_fn=custom_collate_fn)\n",
    "    usps = next(iter(usps_dataloader))\n",
    "\n",
    "    return mnist[0], usps[0]\n",
    "\n",
    "\n",
    "def plot_translations(result_dir):\n",
    "    num_imgs = 50\n",
    "    epochs = 200\n",
    "    _mnist, _usps = get_data(num_imgs)\n",
    "    direc = project_dir + 'domain_adap/cycle_gan/results/' + result_dir + '/'\n",
    "    best_fname = glob(direc + 'epoch*.ckpt')[0]\n",
    "    best_mod = MUCycleGANGP.load_from_checkpoint(best_fname)\n",
    "    see_some_translations(best_mod, _mnist, _usps, 'best', epochs)\n",
    "    mod = MUCycleGANGP.load_from_checkpoint(project_dir + 'domain_adap/cycle_gan/results/' + result_dir + '/last.ckpt')\n",
    "    see_some_translations(mod, _mnist, _usps, 'last', epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819ba5d",
   "metadata": {},
   "source": [
    "### CycleGAN with Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USPSDownSampler(Module):\n",
    "    \"\"\"Common class for down sampling USPS data\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(USPSDownSampler, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "            BatchNorm2d(num_features=8),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in range(0, 7, 3):\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class U2MGenerator(Module):\n",
    "    \"\"\"\n",
    "    Take USPS image [-1, 1] and generate \"fake\" MNIST image [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampler: USPSDownSampler):\n",
    "        super(U2MGenerator, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            # Downsampling part\n",
    "            downsampler,\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=200),\n",
    "            PReLU(num_parameters=200, init=0.25),\n",
    "            BatchNorm1d(num_features=200),\n",
    "\n",
    "            Unflatten(1, (8, 5, 5)),\n",
    "\n",
    "            # Upsampling part\n",
    "            ConvTranspose2d(in_channels=8, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            ConvTranspose2d(in_channels=6, out_channels=3, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            Hardtanh(min_val=-1, max_val=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [2, 6, 9]:\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model._modules['12'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MNISTDownSampler(Module):\n",
    "    \"\"\"Common class for down sampling MNIST data\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MNISTDownSampler, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            Conv2d(in_channels=6, out_channels=8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=8, init=0.25),\n",
    "            BatchNorm2d(num_features=8),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in range(0, 7, 3):\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class M2UGenerator(Module):\n",
    "    \"\"\"\n",
    "    Take MNIST image [-1, 1] and generate \"fake\" USPS image [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampler: MNISTDownSampler):\n",
    "        super(M2UGenerator, self).__init__()\n",
    "\n",
    "        self.model = Sequential(\n",
    "            # Downsampling part\n",
    "            downsampler,\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Linear(in_features=200, out_features=200),\n",
    "            PReLU(num_parameters=200, init=0.25),\n",
    "            BatchNorm1d(num_features=200),\n",
    "\n",
    "            Unflatten(1, (8, 5, 5)),\n",
    "\n",
    "            # Upsampling part\n",
    "            ConvTranspose2d(in_channels=8, out_channels=6, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            PReLU(num_parameters=6, init=0.25),\n",
    "            BatchNorm2d(num_features=6),\n",
    "\n",
    "            ConvTranspose2d(in_channels=6, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            PReLU(num_parameters=3, init=0.25),\n",
    "            BatchNorm2d(num_features=3),\n",
    "\n",
    "            ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            Hardtanh(min_val=-1, max_val=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        for i in [2, 6, 9]:\n",
    "            init.kaiming_normal_(self.model._modules[str(i)].weight, a=0.25, nonlinearity='leaky_relu')\n",
    "        init.xavier_normal_(self.model._modules['12'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CriticM(Module):\n",
    "    \"\"\"\n",
    "    Critic for MNIST images - Checks if the mnist image passed is from mnist \"real\" data distribution or from \"fake\" U2M generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampler: MNISTDownSampler):\n",
    "        super(CriticM, self).__init__()\n",
    "\n",
    "        self.model = Sequential(\n",
    "            downsampler,\n",
    "            Flatten(),\n",
    "            Linear(in_features=200, out_features=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        init.xavier_normal_(self.model._modules['2'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CriticU(Module):\n",
    "    \"\"\"\n",
    "    Critic for USPS images - Checks if the usps image passed is from usps \"real\" data distribution or from \"fake\" M2U generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampler: USPSDownSampler):\n",
    "        super(CriticU, self).__init__()\n",
    "\n",
    "        self.model = Sequential(\n",
    "            downsampler,\n",
    "            Flatten(),\n",
    "            Linear(in_features=200, out_features=1),\n",
    "        )\n",
    "\n",
    "    def initialise(self):\n",
    "        init.xavier_normal_(self.model._modules['2'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MUCycleGAN(LightningModule):\n",
    "\n",
    "    def __init__(self, ncritic, ngen, bs, cycle_weight, clamp_value: float):\n",
    "        super(MUCycleGAN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.ncritic = ncritic\n",
    "        self.ngen = ngen\n",
    "        self.bs = bs\n",
    "        self.cycle_weight = cycle_weight\n",
    "        self.clamp_value = clamp_value\n",
    "\n",
    "        # DownSamplers - Share them between respective generators and critics\n",
    "        self.mnist_downsampler = MNISTDownSampler()\n",
    "        self.usps_downsampler = USPSDownSampler()\n",
    "        self.mnist_downsampler.initialise()\n",
    "        self.usps_downsampler.initialise()\n",
    "\n",
    "        # USPS to MNIST\n",
    "        self.genU2M = U2MGenerator(self.usps_downsampler)\n",
    "        self.criticM = CriticM(self.mnist_downsampler)\n",
    "        self.genU2M.initialise()\n",
    "        self.criticM.initialise()\n",
    "\n",
    "        # MNIST to USPS\n",
    "        self.genM2U = M2UGenerator(self.mnist_downsampler)\n",
    "        self.criticU = CriticU(self.usps_downsampler)\n",
    "        self.genM2U.initialise()\n",
    "        self.criticU.initialise()\n",
    "\n",
    "        # CycleGAN authors use image pool to update the discriminator. That was required because GAN training (on the\n",
    "        # usual objective) was unstable. We are using WGAN and hopefully won't get into such issues. Hence, skipping\n",
    "        # keeping the pool of images\n",
    "\n",
    "        self.float()\n",
    "\n",
    "    def _critic_losses(self, real_uspss, real_mnists, btype):\n",
    "        # WGAN: Critic gets updated from the fake and real data\n",
    "        # CycleGAN: Need to do this for both critics!\n",
    "\n",
    "        # MNIST Critic\n",
    "        real_mnists_score = self.criticM(real_mnists).mean()\n",
    "        fake_mnists = self.genU2M(real_uspss)\n",
    "        fake_mnists_score = self.criticM(fake_mnists).mean()\n",
    "        criticM_loss = fake_mnists_score - real_mnists_score  # minimise this!\n",
    "\n",
    "        # USPS Critic\n",
    "        real_uspss_score = self.criticU(real_uspss).mean()\n",
    "        fake_uspss = self.genM2U(real_mnists)\n",
    "        fake_uspss_score = self.criticU(fake_uspss).mean()\n",
    "        criticU_loss = fake_uspss_score - real_uspss_score\n",
    "\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/criticM_loss', criticM_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/criticU_loss', criticU_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        return criticM_loss, criticU_loss\n",
    "\n",
    "    def _generator_losses(self, real_uspss, real_mnists, btype):\n",
    "        # WGAN: Generator gets updates from the fake data\n",
    "        # CycleGAN: Do this for both generators and additionally put cycle consistency loss\n",
    "\n",
    "        # USPS to MNIST\n",
    "        fake_mnists = self.genU2M(real_uspss)\n",
    "        fake_mnists_score = self.criticM(fake_mnists).mean()\n",
    "        genU2M_loss = -fake_mnists_score  # minimise this!\n",
    "\n",
    "        # MNIST to USPS\n",
    "        fake_uspss = self.genM2U(real_mnists)\n",
    "        fake_uspss_score = self.criticU(fake_uspss).mean()\n",
    "        genM2U_loss = -fake_uspss_score\n",
    "\n",
    "        # Cycle Consistency\n",
    "        # Side Note: *Ideally* L1 norm should be added across dimensions and mean-ed across samples. In their\n",
    "        # implementation, authors have mean-ed across dimensions too, keeping same implementation as them\n",
    "        usps_identity_loss = L1Loss()(real_uspss, self.genM2U(fake_mnists))\n",
    "        mnist_identity_loss = L1Loss()(real_mnists, self.genU2M(fake_uspss))\n",
    "        cycle_loss = self.cycle_weight * (usps_identity_loss + mnist_identity_loss)\n",
    "\n",
    "        not_training = btype != 'train'\n",
    "        self.log(f'{btype}/genU2M_loss', genU2M_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/genM2U_loss', genM2U_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/cycle_loss', cycle_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/usps_identity_loss', usps_identity_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "        self.log(f'{btype}/mnist_identity_loss', mnist_identity_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=not_training)\n",
    "\n",
    "        return genU2M_loss, genM2U_loss, cycle_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real_uspss, real_mnists = batch['usps'][0], batch['mnist'][0]\n",
    "\n",
    "        if optimizer_idx == 1:  # Critic optimizer - only update Critic weights\n",
    "            criticM_loss, criticU_loss = self._critic_losses(real_uspss, real_mnists, 'train')\n",
    "            # CycleGAN authors divide this loss by 2 to slow down the rate of critic learning. Here, as the loss is\n",
    "            # wasserstein loss, I believe it may not be required\n",
    "            return criticM_loss + criticU_loss\n",
    "\n",
    "        if optimizer_idx == 0:  # Generator optimizer - only update Generator weights\n",
    "            genU2M_loss, genM2U_loss, cycle_loss = self._generator_losses(real_uspss, real_mnists, 'train')\n",
    "            return genU2M_loss + genM2U_loss + cycle_loss\n",
    "\n",
    "        # Is there a way to consolidate the losses and return one per epoch?\n",
    "        raise Exception(f'Unknown optimizer index: {optimizer_idx}')\n",
    "\n",
    "    def _shared_eval(self, batch, btype):\n",
    "        real_uspss, real_mnists = batch['usps'][0], batch['mnist'][0]\n",
    "        criticM_loss, criticU_loss = self._critic_losses(real_uspss, real_mnists, btype)\n",
    "        _, _, cycle_loss = self._generator_losses(real_uspss, real_mnists, btype)\n",
    "\n",
    "        mnist_emd = -criticM_loss\n",
    "        usps_emd = -criticU_loss\n",
    "        overall_loss = mnist_emd + usps_emd + cycle_loss\n",
    "\n",
    "        self.log(f'{btype}/mnist_emd', mnist_emd, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=True)\n",
    "        self.log(f'{btype}/usps_emd', usps_emd, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=True)\n",
    "        self.log(f'{btype}/overall_loss', overall_loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, sync_dist=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_eval(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._shared_eval(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Using the strategy from WGAN paper instead of CycleGAN paper!\"\"\"\n",
    "        generator_opt = RMSprop(params=itertools.chain(self.genU2M.parameters(), self.genM2U.parameters()), lr=0.00005)\n",
    "        critic_opt = RMSprop(params=itertools.chain(self.criticU.parameters(), self.criticM.parameters()), lr=0.00005)\n",
    "        return (\n",
    "            {\"optimizer\": generator_opt, \"frequency\": self.ngen},\n",
    "            {\"optimizer\": critic_opt, \"frequency\": self.ncritic},\n",
    "        )\n",
    "\n",
    "    def on_train_batch_end(self, *args):\n",
    "        \"\"\"After weights updating by the optimisers, clamp the weights\"\"\"\n",
    "        for weight in self.criticU.parameters():\n",
    "            weight.data.clamp_(-self.clamp_value, self.clamp_value)\n",
    "        for weight in self.criticM.parameters():\n",
    "            weight.data.clamp_(-self.clamp_value, self.clamp_value)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=True, num_workers=num_cpus,\n",
    "                                     collate_fn=custom_collate_fn)\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=True, num_workers=num_cpus,\n",
    "                                      collate_fn=custom_collate_fn)\n",
    "        return {\n",
    "            'usps': usps_dataloader,\n",
    "            'mnist': mnist_dataloader,\n",
    "        }\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=False,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=False, num_workers=num_cpus,\n",
    "                                     collate_fn=custom_collate_fn)\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=False, num_workers=num_cpus,\n",
    "                                      collate_fn=custom_collate_fn)\n",
    "        return CombinedLoader({\n",
    "            'usps': usps_dataloader,\n",
    "            'mnist': mnist_dataloader,\n",
    "        }, mode='max_size_cycle')\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        usps_dataset = USPS(usps_data_dir, train=False,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                            ]))\n",
    "        usps_dataloader = DataLoader(usps_dataset, self.bs, shuffle=False, num_workers=num_cpus,\n",
    "                                     collate_fn=custom_collate_fn)\n",
    "        mnist_dataset = MNIST(mnist_data_dir, train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),  # Gives a scaled version i.e., 0 to 1\n",
    "                                  transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ]))\n",
    "        mnist_dataloader = DataLoader(mnist_dataset, self.bs, shuffle=False, num_workers=num_cpus,\n",
    "                                      collate_fn=custom_collate_fn)\n",
    "        return CombinedLoader({\n",
    "            'usps': usps_dataloader,\n",
    "            'mnist': mnist_dataloader,\n",
    "        }, mode='max_size_cycle')\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        _summary_kwargs = dict(dtypes=[torch.float], depth=4, col_names=['input_size', 'output_size', 'num_params'],\n",
    "                               row_settings=['depth', 'var_names'], verbose=0, device=torch.device('cpu'))\n",
    "        _usps = torch.randn((10, 1, 16, 16), dtype=torch.float)\n",
    "        _mnist = torch.randn((10, 1, 28, 28), dtype=torch.float)\n",
    "        _summary_string = str(summary(model=self.genU2M, input_data=_usps, **_summary_kwargs)) + '\\n' + \\\n",
    "                          str(summary(model=self.criticU, input_data=_usps, **_summary_kwargs)) + '\\n' + \\\n",
    "                          str(summary(model=self.genM2U, input_data=_mnist, **_summary_kwargs)) + '\\n' + \\\n",
    "                          str(summary(model=self.criticM, input_data=_mnist, **_summary_kwargs))\n",
    "        return _summary_string\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
